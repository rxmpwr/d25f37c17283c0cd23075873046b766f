"""
Data export utilities
"""

import json
import csv
import os
from typing import Dict, List
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


class DataExporter:
    """Handles data export functionality."""
    
    @staticmethod
    def export_to_json(data: dict, filename: str) -> bool:
        """Export data to JSON file."""
        try:
            os.makedirs(os.path.dirname(filename), exist_ok=True) if os.path.dirname(filename) else None
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"Data exported to JSON: {filename}")
            return True
            
        except Exception as e:
            logger.error(f"Error exporting to JSON: {e}")
            return False
    
    @staticmethod
    def export_videos_to_csv(videos: List[dict], filename: str) -> bool:
        """Export video data to CSV file."""
        try:
            os.makedirs(os.path.dirname(filename), exist_ok=True) if os.path.dirname(filename) else None
            
            fieldnames = [
                'video_id', 'title', 'channel_title', 'published_at',
                'view_count', 'like_count', 'comment_count', 'duration',
                'engagement_rate', 'category_id', 'tags'
            ]
            
            with open(filename, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                
                for video in videos:
                    views = video.get('view_count', 1)
                    likes = video.get('like_count', 0)
                    comments = video.get('comment_count', 0)
                    engagement_rate = ((likes + comments) / views) * 100 if views > 0 else 0
                    
                    row = {
                       'video_id': video.get('video_id', ''),
                       'title':'title': video.get('title', ''),
                       'channel_title': video.get('channel_title', ''),
                       'published_at': video.get('published_at', ''),
                       'view_count': video.get('view_count', 0),
                       'like_count': video.get('like_count', 0),
                       'comment_count': video.get('comment_count', 0),
                       'duration': video.get('duration', ''),
                       'engagement_rate': f"{engagement_rate:.2f}",
                       'category_id': video.get('category_id', ''),
                       'tags': ', '.join(video.get('tags', []))
                   }
                   writer.writerow(row)
                   
           logger.info(f"Video data exported to CSV: {filename}")
           return True
           
       except Exception as e:
           logger.error(f"Error exporting to CSV: {e}")
           return False
   
   @staticmethod
   def export_analysis_report(data: dict, filename: str) -> bool:
       """Export comprehensive analysis report."""
       try:
           from analysis_results import format_analysis_results
           
           # Format the analysis
           report_content = format_analysis_results(data)
           
           # Add metadata
           metadata = f"""# YouTube Analysis Report
Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Report Type: Comprehensive Analysis
Data Collection Date: {data.get('collection_date', 'Unknown')}

---

{report_content}

---

## Data Summary
- Channels Analyzed: {len(data.get('channels', []))}
- Videos Analyzed: {len(data.get('video', []))}
- Comments Collected: {len(data.get('bình luận', []))}
- Transcripts Collected: {len(data.get('transcripts', []))}

*This report was generated by YouTube Analyzer Pro*
"""
           
           with open(filename, 'w', encoding='utf-8') as f:
               f.write(metadata)
           
           logger.info(f"Analysis report exported: {filename}")
           return True
           
       except Exception as e:
           logger.error(f"Error exporting analysis report: {e}")
           return False


class CacheManager:
   """Manages data caching for improved performance."""
   
   def __init__(self, cache_dir: str = "cache"):
       self.cache_dir = cache_dir
       os.makedirs(cache_dir, exist_ok=True)
   
   def get_cache_path(self, key: str) -> str:
       """Get cache file path for a key."""
       import re
       safe_key = re.sub(r'[^\w\-_.]', '_', key)
       return os.path.join(self.cache_dir, f"{safe_key}.json")
   
   def get(self, key: str, max_age_hours: int = 24) -> Optional[dict]:
       """Get cached data if it exists and is not expired."""
       from datetime import timedelta
       
       cache_path = self.get_cache_path(key)
       
       if not os.path.exists(cache_path):
           return None
       
       try:
           # Check file age
           file_age = datetime.now() - datetime.fromtimestamp(os.path.getmtime(cache_path))
           if file_age > timedelta(hours=max_age_hours):
               os.remove(cache_path)
               return None
           
           # Load cached data
           with open(cache_path, 'r', encoding='utf-8') as f:
               return json.load(f)
               
       except Exception as e:
           logger.error(f"Error reading cache: {e}")
           return None
   
   def set(self, key: str, data: dict) -> bool:
       """Cache data with timestamp."""
       cache_path = self.get_cache_path(key)
       
       try:
           cache_data = {
               'timestamp': datetime.now().isoformat(),
               'data': data
           }
           
           with open(cache_path, 'w', encoding='utf-8') as f:
               json.dump(cache_data, f, ensure_ascii=False, indent=2, default=str)
           
           return True
           
       except Exception as e:
           logger.error(f"Error writing cache: {e}")
           return False
   
   def clear(self, pattern: str = None) -> int:
       """Clear cache files matching pattern."""
       cleared_count = 0
       
       try:
           for filename in os.listdir(self.cache_dir):
               if filename.endswith('.json'):
                   if pattern is None or pattern in filename:
                       os.remove(os.path.join(self.cache_dir, filename))
                       cleared_count += 1
                       
       except Exception as e:
           logger.error(f"Error clearing cache: {e}")
       
       return cleared_count


class ConfigManager:
   """Manages application configuration."""
   
   def __init__(self, config_file: str = "config/app_config.json"):
       self.config_file = config_file
       self.config = self.load_config()
   
   def load_config(self) -> dict:
       """Load configuration from file."""
       default_config = {
           'ui': {
               'theme': 'light',
               'language': 'en',
               'auto_save': True,
               'show_tooltips': True
           },
           'analysis': {
               'max_videos_per_channel': 20,
               'max_comments_per_video': 50,
               'include_transcripts': True,
               'include_comments': True,
               'viral_threshold': 70
           },
           'export': {
               'default_format': 'json',
               'include_metadata': True,
               'compress_exports': False
           },
           'cache': {
               'enabled': True,
               'max_age_hours': 24,
               'max_size_mb': 100
           }
       }
       
       if not os.path.exists(self.config_file):
           self.save_config(default_config)
           return default_config
       
       try:
           with open(self.config_file, 'r', encoding='utf-8') as f:
               loaded_config = json.load(f)
               
           # Merge with defaults to ensure all keys exist
           merged_config = self._merge_configs(default_config, loaded_config)
           return merged_config
           
       except Exception as e:
           logger.error(f"Error loading config: {e}")
           return default_config
   
   def save_config(self, config: dict = None) -> bool:
       """Save configuration to file."""
       config_to_save = config or self.config
       
       try:
           os.makedirs(os.path.dirname(self.config_file), exist_ok=True)
           
           with open(self.config_file, 'w', encoding='utf-8') as f:
               json.dump(config_to_save, f, ensure_ascii=False, indent=2)
           
           self.config = config_to_save
           return True
           
       except Exception as e:
           logger.error(f"Error saving config: {e}")
           return False
   
   def get(self, key_path: str, default=None):
       """Get configuration value using dot notation."""
       keys = key_path.split('.')
       value = self.config
       
       for key in keys:
           if isinstance(value, dict) and key in value:
               value = value[key]
           else:
               return default
       
       return value
   
   def set(self, key_path: str, value) -> bool:
       """Set configuration value using dot notation."""
       keys = key_path.split('.')
       config = self.config
       
       # Navigate to the parent of the target key
       for key in keys[:-1]:
           if key not in config:
               config[key] = {}
           config = config[key]
       
       # Set the value
       config[keys[-1]] = value
       
       # Save configuration
       return self.save_config()
   
   def _merge_configs(self, default: dict, loaded: dict) -> dict:
       """Recursively merge configurations."""
       merged = default.copy()
       
       for key, value in loaded.items():
           if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
               merged[key] = self._merge_configs(merged[key], value)
           else:
               merged[key] = value
       
       return merged


# Utility functions
def merge_dictionaries(*dicts: dict) -> dict:
   """Merge multiple dictionaries."""
   result = {}
   for d in dicts:
       if isinstance(d, dict):
           result.update(d)
   return result


def flatten_dict(d: dict, parent_key: str = '', sep: str = '.') -> dict:
   """Flatten nested dictionary."""
   items = []
   for k, v in d.items():
       new_key = f"{parent_key}{sep}{k}" if parent_key else k
       if isinstance(v, dict):
           items.extend(flatten_dict(v, new_key, sep=sep).items())
       else:
           items.append((new_key, v))
   return dict(items)


def chunk_list(lst: list, chunk_size: int) -> List[list]:
   """Split list into chunks of specified size."""
   return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]


def is_youtube_url(url: str) -> bool:
   """Quick check if URL is from YouTube."""
   from urllib.parse import urlparse
   
   youtube_domains = ['youtube.com', 'www.youtube.com', 'youtu.be', 'm.youtube.com']
   try:
       parsed = urlparse(url)
       return parsed.hostname in youtube_domains
   except:
       return False


def generate_unique_id(prefix: str = "") -> str:
   """Generate unique identifier."""
   timestamp = int(datetime.now().timestamp() * 1000)
   return f"{prefix}{timestamp}" if prefix else str(timestamp)


def format_file_size(size_bytes: int) -> str:
   """Format file size in human readable format."""
   if size_bytes == 0:
       return "0 B"
   
   size_names = ["B", "KB", "MB", "GB", "TB"]
   i = 0
   
   while size_bytes >= 1024 and i < len(size_names) - 1:
       size_bytes /= 1024.0
       i += 1
   
   return f"{size_bytes:.1f} {size_names[i]}"


def sanitize_filename(filename: str) -> str:
   """Sanitize filename for safe file operations."""
   # Remove invalid characters
   invalid_chars = '<>:"/\\|?*'
   for char in invalid_chars:
       filename = filename.replace(char, '_')
   
   # Truncate if too long
   if len(filename) > 255:
       name, ext = os.path.splitext(filename)
       filename = name[:255-len(ext)] + ext
   
   return filename


# Global instances for convenience
default_cache_manager = CacheManager()
default_config_manager = ConfigManager()()