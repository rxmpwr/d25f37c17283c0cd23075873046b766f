# analysis_results.py
"""
Module for analyzing Youtube data and formatting results in Vietnamese
Provides detailed insights for viral content creation
"""

import re
from typing import Dict, List, Tuple
from collections import Counter
import logging

logger = logging.getLogger(__name__)


def format_analysis_results(data: Dict) -> str:
    """Format analysis results in Vietnamese with detailed insights."""
    if not data:
        return "Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch"
        
    summary = data.get('summary', {})
    
    result_text = f"""
üìä K·∫æT QU·∫¢ PH√ÇN T√çCH YOUTUBE CHI TI·∫æT
{'='*80}

üìà T·ªîNG QUAN D·ªÆ LI·ªÜU:
üì∫ S·ªë k√™nh ph√¢n t√≠ch: {summary.get('channels_analyzed', 0)}
üé¨ T·ªïng s·ªë video: {summary.get('total_videos', 0)}
üí¨ T·ªïng s·ªë b√¨nh lu·∫≠n: {summary.get('total_comments', 0):,}
üìù S·ªë transcript thu th·∫≠p: {summary.get('total_transcripts', 0)}
üëÅÔ∏è T·ªïng l∆∞·ª£t xem: {summary.get('total_views', 0):,}
üëç T·ªïng l∆∞·ª£t th√≠ch: {summary.get('total_likes', 0):,}
üìà T·ª∑ l·ªá t∆∞∆°ng t√°c trung b√¨nh: {summary.get('avg_engagement_rate', 0):.2f}%

"""

    # Ph√¢n t√≠ch t·ª´ng kh√≠a c·∫°nh
    try:
        result_text += analyze_content_themes(data)
        result_text += analyze_us_audience_appeal(data)
        result_text += analyze_audience_sentiment(data)
        result_text += analyze_strengths(data)
        result_text += analyze_weaknesses(data)
        result_text += suggest_viral_strategies(data)
        result_text += show_top_videos_details(data)
    except Exception as e:
        logger.error(f"Error in analysis: {e}")
        result_text += f"\n‚ùå L·ªói khi ph√¢n t√≠ch: {e}\n"
    
    # Additional requirements if any
    additional_reqs = data.get('additional_requirements', [])
    if additional_reqs:
        result_text += f"""
üìã PH√ÇN T√çCH B·ªî SUNG THEO Y√äU C·∫¶U:
{'='*80}

"""
        for i, req in enumerate(additional_reqs, 1):
            result_text += f"üîç Y√™u c·∫ßu {i}: {req['requirement']}\n"
            result_text += f"‚è∞ Th·ªùi gian: {req['timestamp']}\n"
            result_text += f"üìä K·∫øt qu·∫£:\n{req['analysis']}\n\n"
            
    return result_text


def analyze_content_themes(data: Dict) -> str:
    """Ph√¢n t√≠ch ch·ªß ƒë·ªÅ n·ªôi dung qua transcript."""
    transcripts = data.get('transcripts', [])
    videos = data.get('videos', [])
    
    result = f"""
üéØ 1. N·ªòI DUNG C√ÅC VIDEO ƒê·ªÄ C·∫¨P ƒê·∫æN CH·ª¶ ƒê·ªÄ G√å?
{'='*80}

"""
    
    if not transcripts:
        result += "‚ùå Kh√¥ng c√≥ transcript ƒë·ªÉ ph√¢n t√≠ch ch·ªß ƒë·ªÅ.\n"
        result += "üí° G·ª£i √Ω: H√£y ƒë·∫£m b·∫£o video c√≥ ph·ª• ƒë·ªÅ ƒë·ªÉ ph√¢n t√≠ch n·ªôi dung t·ªët h∆°n.\n\n"
        
        # Ph√¢n t√≠ch qua ti√™u ƒë·ªÅ video thay th·∫ø
        if videos:
            result += "üìã PH√ÇN T√çCH QUA TI√äU ƒê·ªÄ VIDEO:\n"
            titles = [video.get('title', '') for video in videos]
            title_themes = analyze_titles_for_themes(titles)
            for theme, count in title_themes.items():
                result += f"  ‚Ä¢ {theme}: {count} video\n"
        result += "\n"
        return result
    
    # Ph√¢n t√≠ch t·ª´ kh√≥a ch√≠nh t·ª´ transcript
    all_text = ""
    video_themes = {}
    
    for transcript in transcripts:
        text = transcript.get('full_text', '')
        all_text += text + " "
        
        # Ph√¢n t√≠ch t·ª´ng video
        video_id = transcript.get('video_id', '')
        if video_id:
            video_themes[video_id] = analyze_text_themes(text)
    
    # T·ª´ kh√≥a t√¢m l√Ω ph·ªï bi·∫øn
    psychology_keywords = {
        'üíï M·ªëi quan h·ªá & T√¨nh y√™u': [
            'relationship', 'love', 'dating', 'partner', 'marriage', 'couple', 
            'romance', 'attraction', 'chemistry', 'soulmate', 'heartbreak'
        ],
        'üß† T√¢m l√Ω h·ªçc & H√†nh vi': [
            'psychology', 'mind', 'brain', 'behavior', 'emotion', 'feeling',
            'subconscious', 'personality', 'trait', 'habit', 'pattern'
        ],
        'üåü Ph√°t tri·ªÉn b·∫£n th√¢n': [
            'self', 'improve', 'growth', 'develop', 'success', 'confidence',
            'motivation', 'goal', 'achieve', 'potential', 'mindset'
        ],
        'üè• S·ª©c kh·ªèe tinh th·∫ßn': [
            'anxiety', 'depression', 'stress', 'mental health', 'therapy',
            'healing', 'trauma', 'recovery', 'wellness', 'peace'
        ],
        'üí¨ Giao ti·∫øp & X√£ h·ªôi': [
            'communication', 'talk', 'speak', 'listen', 'conversation',
            'social', 'friends', 'family', 'connection', 'network'
        ],
        'üî• ƒê·ªông l·ª±c & Truy·ªÅn c·∫£m h·ª©ng': [
            'motivation', 'inspire', 'dream', 'purpose', 'passion',
            'energy', 'drive', 'ambition', 'determination', 'perseverance'
        ],
        'üò® Lo √¢u & S·ª£ h√£i': [
            'fear', 'worry', 'anxiety', 'panic', 'nervous', 'scared',
            'phobia', 'afraid', 'concern', 'insecurity'
        ],
        'üé≠ C·∫£m x√∫c & T√¢m tr·∫°ng': [
            'happy', 'sad', 'angry', 'joy', 'excitement', 'disappointment',
            'grief', 'mood', 'emotional', 'feelings'
        ]
    }
    
    # ƒê·∫øm t·ª´ kh√≥a
    theme_counts = {}
    text_lower = all_text.lower()
    
    for theme, keywords in psychology_keywords.items():
        count = 0
        for keyword in keywords:
            count += text_lower.count(keyword.lower())
        if count > 0:
            theme_counts[theme] = count
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    if theme_counts:
        sorted_themes = sorted(theme_counts.items(), key=lambda x: x[1], reverse=True)
        
        result += "üìã CH·ª¶ ƒê·ªÄ CH√çNH ƒê∆Ø·ª¢C ƒê·ªÄ C·∫¨P (theo m·ª©c ƒë·ªô xu·∫•t hi·ªán):\n\n"
        
        for i, (theme, count) in enumerate(sorted_themes, 1):
            percentage = (count / sum(theme_counts.values())) * 100
            result += f"  {i}. {theme}\n"
            result += f"     üìä Xu·∫•t hi·ªán: {count} l·∫ßn ({percentage:.1f}%)\n"
            
        # Ph√¢n t√≠ch chi ti·∫øt t·ª´ng video
        result += "\nüìù PH√ÇN T√çCH CHI TI·∫æT T·ª™NG VIDEO:\n"
        for i, video in enumerate(videos[:5], 1):  # Ch·ªâ hi·ªÉn th·ªã 5 video ƒë·∫ßu
            video_id = video.get('video_id', '')
            title = video.get('title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')
            result += f"\nüé¨ Video {i}: {title[:80]}...\n"
            
            if video_id in video_themes:
                video_theme_data = video_themes[video_id]
                if video_theme_data:
                    top_theme = max(video_theme_data.items(), key=lambda x: x[1])
                    result += f"   üéØ Ch·ªß ƒë·ªÅ ch√≠nh: {top_theme[0]} ({top_theme[1]} l·∫ßn ƒë·ªÅ c·∫≠p)\n"
                else:
                    result += "   ‚ùì Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c ch·ªß ƒë·ªÅ ch√≠nh\n"
    else:
        result += "‚ùì Kh√¥ng ph√°t hi·ªán ch·ªß ƒë·ªÅ t√¢m l√Ω r√µ r√†ng trong transcript.\n"
        result += "üí° Video c√≥ th·ªÉ ƒë·ªÅ c·∫≠p ƒë·∫øn c√°c ch·ªß ƒë·ªÅ kh√°c ho·∫∑c c·∫ßn ph√¢n t√≠ch s√¢u h∆°n.\n"
    
    result += "\n"
    return result


def analyze_us_audience_appeal(data: Dict) -> str:
    """Ph√¢n t√≠ch s·ª©c h√∫t v·ªõi kh√°n gi·∫£ M·ªπ."""
    result = f"""
üá∫üá∏ 2. ƒêI·ªÄU G√å KHI·∫æN N·ªòI DUNG THU H√öT KH√ÅN GI·∫¢ M·ª∏?
{'='*80}

"""
    
    transcripts = data.get('transcripts', [])
    videos = data.get('videos', [])
    
    if not transcripts:
        result += "‚ùå C·∫ßn transcript ƒë·ªÉ ph√¢n t√≠ch s·ª©c h√∫t v·ªõi kh√°n gi·∫£ M·ªπ.\n\n"
        return result
    
    # Y·∫øu t·ªë thu h√∫t kh√°n gi·∫£ M·ªπ
    us_appeal_factors = {
        'üéØ T√≠nh c√° nh√¢n h√≥a': [
            'you', 'your', 'yourself', 'personal', 'individual', 'own',
            'unique', 'special', 'specific'
        ],
        'üî• T√≠nh kh·∫©n c·∫•p/H·∫•p d·∫´n': [
            'now', 'today', 'immediately', 'urgent', 'important', 'must',
            'need to know', 'secret', 'hidden', 'revealed'
        ],
        'üí™ T·ª± l·ª±c c√°nh sinh': [
            'self-made', 'independent', 'control', 'power', 'strong',
            'overcome', 'achieve', 'success', 'winner'
        ],
        'üß† Khoa h·ªçc/T√¢m l√Ω': [
            'research', 'study', 'science', 'psychology', 'expert',
            'proven', 'facts', 'evidence', 'data'
        ],
        'üí∞ Th√†nh c√¥ng/Ti·ªÅn b·∫°c': [
            'success', 'money', 'rich', 'wealthy', 'profit', 'earn',
            'millionaire', 'achievement', 'goal'
        ],
        '‚ù§Ô∏è C·∫£m x√∫c m·∫°nh': [
            'amazing', 'incredible', 'shocking', 'surprising', 'love',
            'hate', 'fear', 'excited', 'passionate'
        ],
        'üé≠ K·ªÉ chuy·ªán': [
            'story', 'tell', 'happened', 'experience', 'journey',
            'adventure', 'discover', 'reveal'
        ]
    }
    
    # Ph√¢n t√≠ch t·∫•t c·∫£ transcript
    all_text = ""
    for transcript in transcripts:
        all_text += transcript.get('full_text', '') + " "
    
    text_lower = all_text.lower()
    appeal_scores = {}
    
    for factor, keywords in us_appeal_factors.items():
        score = 0
        found_keywords = []
        for keyword in keywords:
            count = text_lower.count(keyword.lower())
            if count > 0:
                score += count
                found_keywords.append(f"{keyword}({count})")
        
        if score > 0:
            appeal_scores[factor] = {'score': score, 'keywords': found_keywords}
    
    if appeal_scores:
        sorted_appeals = sorted(appeal_scores.items(), key=lambda x: x[1]['score'], reverse=True)
        
        result += "üéØ C√ÅC Y·∫æU T·ªê THU H√öT KH√ÅN GI·∫¢ M·ª∏:\n\n"
        
        for i, (factor, data_info) in enumerate(sorted_appeals, 1):
            score = data_info['score']
            keywords = data_info['keywords']
            result += f"  {i}. {factor}\n"
            result += f"     üìä ƒêi·ªÉm s·ªë: {score} (t·ª´ kh√≥a: {', '.join(keywords[:5])})\n"
            
            # Gi·∫£i th√≠ch t·∫°i sao y·∫øu t·ªë n√†y thu h√∫t
            explanations = {
                'üéØ T√≠nh c√° nh√¢n h√≥a': 'Kh√°n gi·∫£ M·ªπ th√≠ch n·ªôi dung t·∫≠p trung v√†o c√° nh√¢n, t·∫°o c·∫£m gi√°c ƒë∆∞·ª£c quan t√¢m ri√™ng.',
                'üî• T√≠nh kh·∫©n c·∫•p/H·∫•p d·∫´n': 'VƒÉn h√≥a "instant gratification" - mu·ªën c√≥ k·∫øt qu·∫£ ngay l·∫≠p t·ª©c.',
                'üí™ T·ª± l·ª±c c√°nh sinh': 'VƒÉn h√≥a t·ª± l·∫≠p m·∫°nh m·∫Ω, tin v√†o kh·∫£ nƒÉng t·ª± thay ƒë·ªïi cu·ªôc ƒë·ªùi.',
                'üß† Khoa h·ªçc/T√¢m l√Ω': 'Tin t∆∞·ªüng v√†o nghi√™n c·ª©u khoa h·ªçc v√† b·∫±ng ch·ª©ng th·ª±c t·∫ø.',
                'üí∞ Th√†nh c√¥ng/Ti·ªÅn b·∫°c': 'ƒê·ªãnh h∆∞·ªõng th√†nh c√¥ng v√† t√†i ch√≠nh r√µ r√†ng.',
                '‚ù§Ô∏è C·∫£m x√∫c m·∫°nh': 'Th√≠ch n·ªôi dung t·∫°o ph·∫£n ·ª©ng c·∫£m x√∫c m·∫°nh m·∫Ω.',
                'üé≠ K·ªÉ chuy·ªán': 'Y√™u th√≠ch c√¢u chuy·ªán c√° nh√¢n v√† tr·∫£i nghi·ªám th·ª±c t·∫ø.'
            }
            
            if factor in explanations:
                result += f"     üí° {explanations[factor]}\n"
            result += "\n"
        
        # ƒê√°nh gi√° t·ªïng th·ªÉ
        total_score = sum(item[1]['score'] for item in sorted_appeals)
        result += f"üìà T·ªîNG ƒêI·ªÇM THU H√öT: {total_score}/100+\n"
        
        if total_score >= 50:
            result += "‚úÖ N·ªôi dung c√≥ ti·ªÅm nƒÉng thu h√∫t t·ªët kh√°n gi·∫£ M·ªπ\n"
        elif total_score >= 20:
            result += "‚ö†Ô∏è N·ªôi dung c√≥ m·ªôt s·ªë y·∫øu t·ªë thu h√∫t, c·∫ßn c·∫£i thi·ªán th√™m\n"
        else:
            result += "‚ùå N·ªôi dung c·∫ßn ƒëi·ªÅu ch·ªânh ƒë·ªÉ ph√π h·ª£p v·ªõi kh√°n gi·∫£ M·ªπ h∆°n\n"
            
    else:
        result += "‚ùì Kh√¥ng ph√°t hi·ªán y·∫øu t·ªë thu h√∫t kh√°n gi·∫£ M·ªπ r√µ r√†ng.\n"
        result += "üí° N√™n th√™m c√°c y·∫øu t·ªë: c√° nh√¢n h√≥a, khoa h·ªçc, t·ª± l·∫≠p, c·∫£m x√∫c m·∫°nh.\n"
    
    result += "\n"
    return result


def analyze_audience_sentiment(data: Dict) -> str:
    """Ph√¢n t√≠ch c·∫£m nh·∫≠n kh√°n gi·∫£ qua comment."""
    result = f"""
üí¨ 3. C·∫¢M NH·∫¨N C·ª¶A KH√ÅN GI·∫¢ KHI XEM VIDEO?
{'='*80}

"""
    
    comments = data.get('comments', [])
    
    if not comments:
        result += "‚ùå Kh√¥ng c√≥ comment ƒë·ªÉ ph√¢n t√≠ch c·∫£m nh·∫≠n kh√°n gi·∫£.\n\n"
        return result
    
    # Ph√¢n lo·∫°i c·∫£m x√∫c trong comment
    sentiment_keywords = {
        'üòç T√≠ch c·ª±c/Th√≠ch th√∫': [
            'love', 'amazing', 'great', 'awesome', 'fantastic', 'excellent',
            'perfect', 'wonderful', 'brilliant', 'outstanding', 'incredible',
            'thank you', 'helpful', 'useful', 'inspiring', 'motivating'
        ],
        'üòî Ti√™u c·ª±c/Kh√¥ng th√≠ch': [
            'hate', 'terrible', 'awful', 'bad', 'worst', 'boring',
            'stupid', 'waste', 'disappointed', 'disagree', 'wrong'
        ],
        'ü§î Th·∫Øc m·∫Øc/T√≤ m√≤': [
            'question', 'how', 'why', 'what', 'when', 'where',
            'curious', 'wonder', 'confused', 'explain', 'help'
        ],
        'üî• H·ª©ng th√∫/K√≠ch ƒë·ªông': [
            'excited', 'wow', 'omg', 'amazing', 'mind blown',
            'shocking', 'unbelievable', 'crazy', 'insane'
        ],
        'üéØ Li√™n quan c√° nh√¢n': [
            'me too', 'same here', 'relate', 'exactly', 'my life',
            'happened to me', 'i feel', 'my experience'
        ],
        'üôè Bi·∫øt ∆°n/C·∫£m k√≠ch': [
            'thank', 'grateful', 'appreciate', 'helped me',
            'changed my life', 'saved me', 'blessing'
        ]
    }
    
    # Ph√¢n t√≠ch sentiment
    sentiment_counts = {}
    all_comments_text = ""
    
    for comment in comments:
        text = comment.get('text', '').lower()
        all_comments_text += text + " "
        
        for sentiment, keywords in sentiment_keywords.items():
            if sentiment not in sentiment_counts:
                sentiment_counts[sentiment] = 0
            
            for keyword in keywords:
                if keyword.lower() in text:
                    sentiment_counts[sentiment] += 1
    
    # Ph√¢n t√≠ch top comment
    top_comments = sorted(comments, key=lambda x: x.get('like_count', 0), reverse=True)[:10]
    
    result += "üìä PH√ÇN T√çCH C·∫¢M X√öC TRONG COMMENT:\n\n"
    
    if sentiment_counts:
        total_sentiments = sum(sentiment_counts.values())
        sorted_sentiments = sorted(sentiment_counts.items(), key=lambda x: x[1], reverse=True)
        
        for sentiment, count in sorted_sentiments:
            if count > 0:
                percentage = (count / total_sentiments) * 100
                result += f"  {sentiment}: {count} comment ({percentage:.1f}%)\n"
        
        # ƒê√°nh gi√° t·ªïng th·ªÉ
        positive_score = sentiment_counts.get('üòç T√≠ch c·ª±c/Th√≠ch th√∫', 0) + sentiment_counts.get('üôè Bi·∫øt ∆°n/C·∫£m k√≠ch', 0)
        negative_score = sentiment_counts.get('üòî Ti√™u c·ª±c/Kh√¥ng th√≠ch', 0)
        
        result += f"\nüìà ƒê√ÅNH GI√Å T·ªîNG TH·ªÇ:\n"
        if positive_score > negative_score * 2:
            result += "‚úÖ Ph·∫£n ·ª©ng kh√°n gi·∫£ r·∫•t t√≠ch c·ª±c\n"
        elif positive_score > negative_score:
            result += "üëç Ph·∫£n ·ª©ng kh√°n gi·∫£ t√≠ch c·ª±c\n"
        elif negative_score > positive_score:
            result += "üëé C√≥ m·ªôt s·ªë ph·∫£n ·ª©ng ti√™u c·ª±c c·∫ßn l∆∞u √Ω\n"
        else:
            result += "üòê Ph·∫£n ·ª©ng kh√°n gi·∫£ trung t√≠nh\n"
    
    # Hi·ªÉn th·ªã top comment
    result += "\nüîù TOP COMMENT C√ì NHI·ªÄU LIKE NH·∫§T:\n\n"
    for i, comment in enumerate(top_comments[:5], 1):
        text = comment.get('text', '')[:150]
        if len(comment.get('text', '')) > 150:
            text += '...'
        likes = comment.get('like_count', 0)
        author = comment.get('author', 'Anonymous')
        
        result += f"  {i}. üë§ {author} (üëç {likes})\n"
        result += f"     üí¨ \"{text}\"\n\n"
    
    result += "\n"
    return result


def analyze_strengths(data: Dict) -> str:
    """Ph√¢n t√≠ch ƒëi·ªÉm m·∫°nh c·ªßa video."""
    result = f"""
üí™ 4. ƒêI·ªÇM M·∫†NH C·ª¶A C√ÅC VIDEO:
{'='*80}

"""
    
    videos = data.get('videos', [])
    comments = data.get('comments', [])
    summary = data.get('summary', {})
    
    strengths = []
    
    # Ph√¢n t√≠ch engagement rate
    avg_engagement = summary.get('avg_engagement_rate', 0)
    if avg_engagement > 5:
        strengths.append(f"üî• T·ª∑ l·ªá t∆∞∆°ng t√°c xu·∫•t s·∫Øc ({avg_engagement:.2f}%) - Top 1% tr√™n YouTube")
    elif avg_engagement > 2:
        strengths.append(f"üìà T·ª∑ l·ªá t∆∞∆°ng t√°c t·ªët ({avg_engagement:.2f}%) - Tr√™n m·ª©c trung b√¨nh ng√†nh")
    
    # Ph√¢n t√≠ch l∆∞·ª£t xem
    if videos:
        total_views = sum(video.get('view_count', 0) for video in videos)
        avg_views = total_views / len(videos)
        
        # Check viral videos
        viral_videos = [v for v in videos if v.get('view_count', 0) > 1000000]
        if viral_videos:
            strengths.append(f"üöÄ {len(viral_videos)} video ƒë·∫°t tri·ªáu views - Kh·∫£ nƒÉng t·∫°o viral content")
        elif avg_views > 100000:
            strengths.append(f"üëÅÔ∏è L∆∞·ª£t xem ·∫•n t∆∞·ª£ng (TB: {avg_views:,.0f} l∆∞·ª£t/video)")
        elif avg_views > 10000:
            strengths.append(f"üì∫ L∆∞·ª£t xem t·ªët (TB: {avg_views:,.0f} l∆∞·ª£t/video)")
    
    # Ph√¢n t√≠ch comment quality
    if comments:
        long_comments = [c for c in comments if len(c.get('text', '')) > 50]
        engaging_comments = [c for c in comments if any(word in c.get('text', '').lower() 
                           for word in ['thank', 'helpful', 'love', 'great'])]
        
        if len(long_comments) / len(comments) > 0.3:
            strengths.append("üí¨ Comment ch·∫•t l∆∞·ª£ng cao - Kh√°n gi·∫£ engage s√¢u v·ªõi content")
        if len(engaging_comments) / len(comments) > 0.5:
            strengths.append("‚ù§Ô∏è Kh√°n gi·∫£ y√™u th√≠ch content - Nhi·ªÅu ph·∫£n h·ªìi t√≠ch c·ª±c")
    
    # Ph√¢n t√≠ch consistency
    if len(videos) > 3:
        view_counts = [v.get('view_count', 0) for v in videos]
        if view_counts:
            avg = sum(view_counts) / len(view_counts)
            consistent_videos = [v for v in view_counts if v > avg * 0.5]
            if len(consistent_videos) > len(view_counts) * 0.7:
                strengths.append("üìä Performance ·ªïn ƒë·ªãnh - ƒê√£ c√≥ audience base trung th√†nh")
    
    # Ph√¢n t√≠ch growth trend
    if videos and len(videos) > 5:
        recent_views = sum(v.get('view_count', 0) for v in videos[:5]) / 5
        older_views = sum(v.get('view_count', 0) for v in videos[-5:]) / 5
        if recent_views > older_views * 1.5:
            growth_percent = ((recent_views - older_views) / older_views) * 100
            strengths.append(f"üìà Channel ƒëang growth m·∫°nh (+{growth_percent:.0f}% views)")
    
    # Hi·ªÉn th·ªã ƒëi·ªÉm m·∫°nh
    if strengths:
        for i, strength in enumerate(strengths, 1):
            result += f"  {i}. {strength}\n"
    else:
        result += "‚ùì C·∫ßn ph√¢n t√≠ch th√™m ƒë·ªÉ x√°c ƒë·ªãnh ƒëi·ªÉm m·∫°nh c·ª• th·ªÉ.\n"
    
    # Ph√¢n t√≠ch video performance cao nh·∫•t
    if videos:
        top_video = max(videos, key=lambda x: x.get('view_count', 0))
        result += f"\nüèÜ VIDEO HI·ªÜU SU·∫§T CAO NH·∫§T:\n"
        result += f"   üìπ {top_video.get('title', '')[:80]}...\n"
        result += f"   üëÅÔ∏è {top_video.get('view_count', 0):,} l∆∞·ª£t xem\n"
        result += f"   üëç {top_video.get('like_count', 0):,} l∆∞·ª£t th√≠ch\n"
        
        engagement = 0
        if top_video.get('view_count', 0) > 0:
            engagement = ((top_video.get('like_count', 0) + top_video.get('comment_count', 0)) / 
                         top_video.get('view_count', 0)) * 100
        result += f"   üìà T·ª∑ l·ªá t∆∞∆°ng t√°c: {engagement:.2f}%\n"
        
        # Ph√¢n t√≠ch y·∫øu t·ªë th√†nh c√¥ng
        result += f"\n   üîç Y·∫æU T·ªê TH√ÄNH C√îNG:\n"
        title = top_video.get('title', '').lower()
        if any(word in title for word in ['how', 'why', 'what']):
            result += "   ‚Ä¢ S·ª≠ d·ª•ng question hook hi·ªáu qu·∫£\n"
        if any(char.isdigit() for char in title):
            result += "   ‚Ä¢ C√≥ s·ªë trong title (listicle format)\n"
        if '?' in title:
            result += "   ‚Ä¢ T·∫°o curiosity v·ªõi c√¢u h·ªèi\n"
    
    result += "\n"
    return result


def analyze_weaknesses(data: Dict) -> str:
    """Ph√¢n t√≠ch ƒëi·ªÉm h·∫°n ch·∫ø v√† c·∫ßn c·∫£i thi·ªán."""
    result = f"""
‚ö†Ô∏è 5. ƒêI·ªÇM H·∫†N CH·∫æ V√Ä C·∫¶N C·∫¢I THI·ªÜN:
{'='*80}

"""
    
    videos = data.get('videos', [])
    comments = data.get('comments', [])
    transcripts = data.get('transcripts', [])
    summary = data.get('summary', {})
    
    weaknesses = []
    improvements = []
    
    # Ph√¢n t√≠ch engagement th·∫•p
    avg_engagement = summary.get('avg_engagement_rate', 0)
    if avg_engagement < 1:
        weaknesses.append("üìâ T·ª∑ l·ªá t∆∞∆°ng t√°c th·∫•p - D∆∞·ªõi 1% l√† m·ª©c b√°o ƒë·ªông")
        improvements.append("üí° Hook m·∫°nh h∆°n trong 3 gi√¢y ƒë·∫ßu + CTA r√µ r√†ng")
    elif avg_engagement < 2:
        weaknesses.append("üìä T·ª∑ l·ªá t∆∞∆°ng t√°c d∆∞·ªõi trung b√¨nh ng√†nh")
        improvements.append("üí° Test nhi·ªÅu format content ƒë·ªÉ t√¨m style ph√π h·ª£p")
    
    # Ph√¢n t√≠ch comment √≠t
    if videos:
        total_comments = sum(v.get('comment_count', 0) for v in videos)
        avg_comments = total_comments / len(videos)
        if avg_comments < 50:
            weaknesses.append("üí¨ √çt comment - Kh√°n gi·∫£ ch∆∞a c√≥ ƒë·ªông l·ª±c t∆∞∆°ng t√°c")
            improvements.append("üí° ƒê·∫∑t c√¢u h·ªèi c·ª• th·ªÉ cu·ªëi video + pin comment h·∫•p d·∫´n")
    
    # Ph√¢n t√≠ch thi·∫øu transcript
    if len(transcripts) < len(videos) * 0.5:
        missing_percent = ((len(videos) - len(transcripts)) / len(videos)) * 100
        weaknesses.append(f"üìù {missing_percent:.0f}% video thi·∫øu transcript - M·∫•t SEO v√† accessibility")
        improvements.append("üí° Upload subtitle cho t·∫•t c·∫£ video ƒë·ªÉ tƒÉng reach")
    
    # Ph√¢n t√≠ch ƒë·ªô d√†i title
    if videos:
        long_titles = [v for v in videos if len(v.get('title', '')) > 100]
        short_titles = [v for v in videos if len(v.get('title', '')) < 30]
        
        if len(long_titles) > len(videos) * 0.3:
            weaknesses.append("üìù Nhi·ªÅu title qu√° d√†i - B·ªã c·∫Øt tr√™n mobile")
            improvements.append("üí° Gi·ªõi h·∫°n title 60-70 k√Ω t·ª±, keyword quan tr·ªçng ƒë·∫ßu ti√™n")
        if len(short_titles) > len(videos) * 0.3:
            weaknesses.append("üìù Nhi·ªÅu title qu√° ng·∫Øn - Thi·∫øu keyword SEO")
            improvements.append("üí° M·ªü r·ªông title v·ªõi keyword relevant")
    
    # Ph√¢n t√≠ch thumbnail (d·ª±a v√†o performance)
    if videos and len(videos) > 5:
        # Check videos c√≥ performance th·∫•p b·∫•t th∆∞·ªùng
        avg_views = sum(v.get('view_count', 0) for v in videos) / len(videos)
        poor_performers = [v for v in videos if v.get('view_count', 0) < avg_views * 0.3]
        if len(poor_performers) > len(videos) * 0.3:
            weaknesses.append("üñºÔ∏è Nhi·ªÅu video underperform - C√≥ th·ªÉ do thumbnail k√©m h·∫•p d·∫´n")
            improvements.append("üí° A/B test thumbnail v·ªõi m√†u s·∫Øc t∆∞∆°ng ph·∫£n + text r√µ r√†ng")
    
    # Ph√¢n t√≠ch posting schedule
    if videos and len(videos) > 3:
        # Simple check for consistency (can be improved)
        weaknesses.append("üìÖ C·∫ßn review posting schedule")
        improvements.append("üí° Post ƒë·ªÅu ƒë·∫∑n v√†o gi·ªù v√†ng c·ªßa target audience")
    
    # Ph√¢n t√≠ch negative comments
    if comments:
        negative_words = ['bad', 'terrible', 'worst', 'hate', 'boring', 'stupid', 'waste']
        negative_comments = []
        for comment in comments:
            text = comment.get('text', '').lower()
            if any(word in text for word in negative_words):
                negative_comments.append(comment)
        
        if len(negative_comments) > len(comments) * 0.1:  # Tr√™n 10% comment ti√™u c·ª±c
            weaknesses.append(f"üëé {len(negative_comments)} comment ti√™u c·ª±c - C·∫ßn address concerns")
            improvements.append("üí° Ph√¢n t√≠ch feedback pattern v√† c·∫£i thi·ªán weak points")
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    if weaknesses:
        result += "üö® C√ÅC V·∫§N ƒê·ªÄ C·∫¶N KH·∫ÆC PH·ª§C:\n\n"
        for i, weakness in enumerate(weaknesses, 1):
            result += f"  {i}. {weakness}\n"
        
        result += "\nüîß G·ª¢I √ù C·∫¢I THI·ªÜN C·ª§ TH·ªÇ:\n\n"
        for i, improvement in enumerate(improvements, 1):
            result += f"  {i}. {improvement}\n"
    else:
        result += "‚úÖ Kh√¥ng ph√°t hi·ªán ƒëi·ªÉm y·∫øu ƒë√°ng k·ªÉ.\n"
        result += "üí° Ti·∫øp t·ª•c optimize v√† scale nh·ªØng g√¨ ƒëang work.\n"
    
    result += "\n"
    return result


def suggest_viral_strategies(data: Dict) -> str:
    """G·ª£i √Ω chi·∫øn l∆∞·ª£c viral."""
    result = f"""
üöÄ 6. CHI·∫æN L∆Ø·ª¢C VIRAL - TI·∫æP C·∫¨N NHI·ªÄU KH√ÅN GI·∫¢ H∆†N:
{'='*80}

"""
    
    videos = data.get('videos', [])
    transcripts = data.get('transcripts', [])
    comments = data.get('comments', [])
    summary = data.get('summary', {})
    
    # Chi·∫øn l∆∞·ª£c n·ªôi dung d·ª±a tr√™n data
    result += "üìù CHI·∫æN L∆Ø·ª¢C N·ªòI DUNG:\n"
    
    if videos:
        # Ph√¢n t√≠ch top performers
        top_videos = sorted(videos, key=lambda x: x.get('view_count', 0), reverse=True)[:5]
        common_elements = analyze_common_elements(top_videos)
        
        if common_elements:
            result += f"   ‚Ä¢ Pattern th√†nh c√¥ng: {', '.join(common_elements)}\n"
            result += "   ‚Ä¢ Scale nh·ªØng y·∫øu t·ªë n√†y trong content m·ªõi\n"
        else:
            result += "   ‚Ä¢ Test nhi·ªÅu format ƒë·ªÉ t√¨m winning formula\n"
    
    result += "   ‚Ä¢ Hook m·∫°nh m·∫Ω trong 3-5 gi√¢y ƒë·∫ßu\n"
    result += "   ‚Ä¢ Story structure: Problem ‚Üí Journey ‚Üí Solution ‚Üí Transformation\n"
    
    # Chi·∫øn l∆∞·ª£c ti√™u ƒë·ªÅ d·ª±a tr√™n performance
    result += "\nüéØ CHI·∫æN L∆Ø·ª¢C TI√äU ƒê·ªÄ VIRAL:\n"
    
    if videos:
        # Analyze successful title patterns
        high_view_videos = [v for v in videos if v.get('view_count', 0) > 
                           sum(v.get('view_count', 0) for v in videos) / len(videos)]
        
        title_patterns = []
        for video in high_view_videos:
            title = video.get('title', '').lower()
            if '?' in title:
                title_patterns.append("Questions")
            if any(str(i) in title for i in range(10)):
                title_patterns.append("Numbers")
            if any(word in title for word in ['secret', 'truth', 'hidden']):
                title_patterns.append("Mystery/Curiosity")
                
        if title_patterns:
            most_common = Counter(title_patterns).most_common(2)
            result += f"   ‚Ä¢ Patterns work t·ªët: {', '.join([p[0] for p in most_common])}\n"
    
    result += "   ‚Ä¢ Formula: [Number] + [Emotional Word] + [Benefit] + [Curiosity Gap]\n"
    result += "   ‚Ä¢ A/B test 3-5 variations cho m·ªói video\n"
    
    # Chi·∫øn l∆∞·ª£c thumbnail
    result += "\nüñºÔ∏è CHI·∫æN L∆Ø·ª¢C THUMBNAIL:\n"
    result += "   ‚Ä¢ Contrast cao + M√†u s·∫Øc n·ªïi b·∫≠t (ƒë·ªè, v√†ng, xanh neon)\n"
    result += "   ‚Ä¢ Face closeup v·ªõi emotion m·∫°nh\n"
    result += "   ‚Ä¢ Text to ƒë·∫≠m, d·ªÖ ƒë·ªçc tr√™n mobile\n"
    result += "   ‚Ä¢ Test v·ªõi/kh√¥ng arrow v√† circle\n"
    
    # Chi·∫øn l∆∞·ª£c t√¢m l√Ω
    result += "\nüß† CHI·∫æN L∆Ø·ª¢C T√ÇM L√ù KH√ÅN GI·∫¢:\n"
    
    if comments:
        # Analyze what resonates with audience
        positive_comments = [c for c in comments if any(word in c.get('text', '').lower() 
                           for word in ['helpful', 'thank', 'love', 'relate'])]
        
        if positive_comments:
            result += "   ‚Ä¢ Audience respond t·ªët v·ªõi content c√≥ t√≠nh relate cao\n"
            result += "   ‚Ä¢ Focus v√†o personal stories v√† real experiences\n"
        else:
            result += "   ‚Ä¢ Test emotional triggers kh√°c nhau\n"
            result += "   ‚Ä¢ T·∫°o connection qua vulnerability v√† authenticity\n"
    
    # Platform strategy d·ª±a tr√™n data
    result += "\nüì± CHI·∫æN L∆Ø·ª¢C PLATFORM:\n"
    
    avg_views = sum(v.get('view_count', 0) for v in videos) / len(videos) if videos else 0
    
    if avg_views < 10000:
        result += "   ‚Ä¢ Focus YouTube SEO: Tags, descriptions, keywords\n"
        result += "   ‚Ä¢ Leverage YouTube Shorts cho discoverability\n"
    elif avg_views < 100000:
        result += "   ‚Ä¢ Cross-promote tr√™n TikTok/Instagram Reels\n"
        result += "   ‚Ä¢ Collaborate v·ªõi channels c√πng size\n"
    else:
        result += "   ‚Ä¢ Scale winning content across platforms\n"
        result += "   ‚Ä¢ Build email list t·ª´ top viewers\n"
    
    # Timing strategy
    result += "\n‚è∞ CHI·∫æN L∆Ø·ª¢C TH·ªúI GIAN:\n"
    result += "   ‚Ä¢ Post gi·ªù v√†ng: 14:00-16:00 v√† 20:00-22:00 (target timezone)\n"
    result += "   ‚Ä¢ Consistency quan tr·ªçng h∆°n frequency\n"
    result += "   ‚Ä¢ Ride trending topics khi relevant\n"
    
    # PH·∫¶N H√ÄNH ƒê·ªòNG TI·∫æP THEO - DYNAMIC
    result += f"\nüé¨ H√ÄNH ƒê·ªòNG TI·∫æP THEO (D·ª±a tr√™n ph√¢n t√≠ch k√™nh n√†y):\n"
    
    # T·∫°o action items ƒë·ªông
    action_items = generate_dynamic_action_items(data)
    
    # Hi·ªÉn th·ªã t·ªëi ƒëa 5 action items quan tr·ªçng nh·∫•t
    for i, action in enumerate(action_items[:5], 1):
        result += f"   {i}. {action}\n"
    
    # M·ª•c ti√™u c·ª• th·ªÉ
    result += f"\nüìä M·ª§C TI√äU C·ª§ TH·ªÇ CHO 30 NG√ÄY T·ªöI:\n"
    
    # T√≠nh to√°n m·ª•c ti√™u d·ª±a tr√™n current performance
    if videos:
        current_avg_views = sum(v.get('view_count', 0) for v in videos[:10]) / min(len(videos), 10)
        current_avg_engagement = summary.get('avg_engagement_rate', 0)
        
        # Set realistic growth targets
        if current_avg_views < 1000:
            target_views = 2500
            growth_text = "2.5x growth"
        elif current_avg_views < 10000:
            target_views = int(current_avg_views * 2)
            growth_text = "2x growth"
        elif current_avg_views < 100000:
            target_views = int(current_avg_views * 1.5)
            growth_text = "50% growth"
        else:
            target_views = int(current_avg_views * 1.3)
            growth_text = "30% growth"
            
        target_engagement = max(current_avg_engagement * 1.2, 3.0)
        target_subscribers = max(100, int(current_avg_views / 100))
        
        result += f"   ‚Ä¢ Average views: {current_avg_views:,.0f} ‚Üí {target_views:,} ({growth_text})\n"
        result += f"   ‚Ä¢ Engagement rate: {current_avg_engagement:.1f}% ‚Üí {target_engagement:.1f}%\n"
        result += f"   ‚Ä¢ New subscribers: +{target_subscribers:,} subscribers\n"
    else:
        result += "   ‚Ä¢ Establish baseline metrics trong tu·∫ßn ƒë·∫ßu\n"
        result += "   ‚Ä¢ Target 1,000 views/video trong th√°ng ƒë·∫ßu\n"
        result += "   ‚Ä¢ Build ƒë·∫øn 100 loyal subscribers\n"
    
    result += "\n"
    return result


def show_top_videos_details(data: Dict) -> str:
    """Hi·ªÉn th·ªã chi ti·∫øt video top performance."""
    result = f"""
üèÜ 7. CHI TI·∫æT VIDEO TOP PERFORMANCE:
{'='*80}

"""
    
    videos = data.get('videos', [])
    
    if not videos:
        result += "‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu video ƒë·ªÉ ph√¢n t√≠ch.\n"
        return result
    
    # S·∫Øp x·∫øp video theo view count
    sorted_videos = sorted(videos, key=lambda x: x.get('view_count', 0), reverse=True)
    
    for i, video in enumerate(sorted_videos[:5], 1):  # Top 5 videos
        title = video.get('title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')
        views = video.get('view_count', 0)
        likes = video.get('like_count', 0)
        comments = video.get('comment_count', 0)
        published = video.get('published_at', '')[:10]
        duration = video.get('duration', '')
        
        # T√≠nh engagement rate
        engagement_rate = 0
        if views > 0:
            engagement_rate = ((likes + comments) / views) * 100
        
        # Ph√¢n t√≠ch title
        title_analysis = analyze_title_viral_potential(title)
        
        result += f"üé¨ TOP {i}: {title}\n"
        result += f"   üìä {views:,} views | {likes:,} likes | {comments:,} comments\n"
        result += f"   üìà Engagement: {engagement_rate:.2f}% | Ng√†y ƒëƒÉng: {published}"
        if duration:
            result += f" | ƒê·ªô d√†i: {format_duration(duration)}"
        result += f"\n   üéØ Ph√¢n t√≠ch ti√™u ƒë·ªÅ: {title_analysis}\n"
        
        # ƒê√°nh gi√° performance  
        performance_emoji = ""
        if engagement_rate > 5:
            performance_emoji = "üî•"
            performance_text = "Performance xu·∫•t s·∫Øc - Top 1%"
        elif engagement_rate > 2:
            performance_emoji = "‚≠ê"
            performance_text = "Performance t·ªët - Tr√™n trung b√¨nh"
        elif engagement_rate > 1:
            performance_emoji = "üëç"
            performance_text = "Performance trung b√¨nh"
        else:
            performance_emoji = "üìä"
            performance_text = "C·∫ßn c·∫£i thi·ªán engagement"
            
        result += f"   {performance_emoji} {performance_text}\n"
        
        # Success factors cho top performer
        if i == 1 and engagement_rate > 2:
            result += "\n   üîë Y·∫æU T·ªê TH√ÄNH C√îNG C·ª¶A VIDEO N√ÄY:\n"
            success_factors = analyze_video_success_factors(video)
            for factor in success_factors:
                result += f"      ‚Ä¢ {factor}\n"
        
        result += "\n"
    
    # Overall insights
    if len(videos) > 5:
        result += "üí° INSIGHTS T·ª™ TOP PERFORMERS:\n"
        insights = generate_performance_insights(sorted_videos[:5])
        for insight in insights:
            result += f"   ‚Ä¢ {insight}\n"
    
    return result


# Helper functions
def analyze_text_themes(text: str) -> Dict[str, int]:
    """Ph√¢n t√≠ch ch·ªß ƒë·ªÅ t·ª´ text."""
    themes = {
        'T√¢m l√Ω': ['psychology', 'mind', 'brain', 'mental', 'emotion'],
        'M·ªëi quan h·ªá': ['relationship', 'love', 'partner', 'couple', 'dating'],
        'Ph√°t tri·ªÉn b·∫£n th√¢n': ['self', 'improve', 'growth', 'success', 'confidence'],
        'C·∫£m x√∫c': ['feel', 'emotion', 'happy', 'sad', 'anger', 'fear'],
        'H√†nh vi': ['behavior', 'action', 'habit', 'pattern', 'react']
    }
    
    text_lower = text.lower()
    theme_counts = {}
    
    for theme, keywords in themes.items():
        count = sum(text_lower.count(keyword) for keyword in keywords)
        if count > 0:
            theme_counts[theme] = count
    
    return theme_counts


def analyze_titles_for_themes(titles: List[str]) -> Dict[str, int]:
    """Ph√¢n t√≠ch ch·ªß ƒë·ªÅ t·ª´ danh s√°ch ti√™u ƒë·ªÅ."""
    all_titles = ' '.join(titles).lower()
    
    themes = {
        'T√¢m l√Ω h·ªçc': ['psychology', 'mind', 'brain', 'mental'],
        'M·ªëi quan h·ªá': ['relationship', 'love', 'dating', 'couple'],
        'C·∫£m x√∫c': ['emotion', 'feel', 'happy', 'sad'],
        'H√†nh vi': ['behavior', 'action', 'habit'],
        'Ph√°t tri·ªÉn': ['growth', 'improve', 'success']
    }
    
    theme_counts = {}
    for theme, keywords in themes.items():
        count = sum(all_titles.count(keyword) for keyword in keywords)
        if count > 0:
            theme_counts[theme] = count
    
    return theme_counts


def analyze_title_viral_potential(title: str) -> str:
    """Ph√¢n t√≠ch ti·ªÅm nƒÉng viral c·ªßa ti√™u ƒë·ªÅ."""
    viral_indicators = {
        'S·ªë': any(char.isdigit() for char in title),
        'C√¢u h·ªèi': '?' in title,
        'C·∫£m x√∫c m·∫°nh': any(word in title.lower() for word in ['shocking', 'amazing', 'incredible', 'secret']),
        'T√≠nh c√° nh√¢n': any(word in title.lower() for word in ['you', 'your']),
        'T√≠nh kh·∫©n c·∫•p': any(word in title.lower() for word in ['now', 'immediately', 'today']),
        'T√≠nh ƒë·ªôc quy·ªÅn': any(word in title.lower() for word in ['secret', 'hidden', 'never', 'nobody'])
    }
    
    score = sum(1 for indicator in viral_indicators.values() if indicator)
    
    if score >= 4:
        return "Ti·ªÅm nƒÉng viral r·∫•t cao ‚≠ê‚≠ê‚≠ê‚≠ê"
    elif score >= 3:
        return "Ti·ªÅm nƒÉng viral cao ‚≠ê‚≠ê‚≠ê"
    elif score >= 2:
        return "Ti·ªÅm nƒÉng viral trung b√¨nh ‚≠ê‚≠ê"
    elif score >= 1:
        return "Ti·ªÅm nƒÉng viral th·∫•p ‚≠ê"
    else:
        return "C·∫ßn t·ªëi ∆∞u title ƒë·ªÉ viral"


def format_duration(duration: str) -> str:
    """Format duration t·ª´ YouTube API (PT4M13S) th√†nh readable."""
    import re
    match = re.match(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', duration)
    if not match:
        return duration
        
    hours, minutes, seconds = match.groups()
    hours = int(hours) if hours else 0
    minutes = int(minutes) if minutes else 0
    seconds = int(seconds) if seconds else 0
    
    if hours > 0:
        return f"{hours}:{minutes:02d}:{seconds:02d}"
    else:
        return f"{minutes}:{seconds:02d}"


def analyze_common_elements(videos: List[Dict]) -> List[str]:
    """T√¨m elements chung trong top videos."""
    elements = []
    
    if not videos:
        return elements
        
    # Check title patterns
    titles = [v.get('title', '').lower() for v in videos]
    
    # Common words (exclude common ones)
    all_words = ' '.join(titles).split()
    word_freq = Counter(word for word in all_words 
                       if len(word) > 4 and word not in ['about', 'video', 'youtube'])
    
    common_words = [word for word, count in word_freq.most_common(3) if count > len(videos) * 0.3]
    if common_words:
        elements.append(f"Keywords: {', '.join(common_words)}")
    
    # Check format patterns
    if sum(1 for t in titles if '?' in t) > len(titles) * 0.5:
        elements.append("Question format")
    if sum(1 for t in titles if any(char.isdigit() for char in t)) > len(titles) * 0.5:
        elements.append("Numbers in title")
        
    return elements


def generate_dynamic_action_items(data: Dict) -> List[str]:
    """Generate dynamic action items based on analysis."""
    action_items = []
    videos = data.get('videos', [])
    comments = data.get('comments', [])
    transcripts = data.get('transcripts', [])
    summary = data.get('summary', {})
    
    if not videos:
        return ["üìä Thu th·∫≠p data ƒë·ªÉ c√≥ insights c·ª• th·ªÉ", 
                "üéØ B·∫Øt ƒë·∫ßu v·ªõi 5 test videos kh√°c nhau"]
    
    # 1. Based on view performance
    avg_views = sum(v.get('view_count', 0) for v in videos) / len(videos)
    
    if avg_views < 1000:
        action_items.append("üîç Nghi√™n c·ª©u SEO: Optimize title, tags, descriptions v·ªõi target keywords")
        action_items.append("üñºÔ∏è Redesign thumbnails: A/B test v·ªõi colors v√† text r√µ r√†ng")
    elif avg_views < 10000:
        action_items.append("üì± T·∫°o YouTube Shorts t·ª´ best moments c·ªßa long videos")
        action_items.append("üéØ Target specific niche keywords thay v√¨ broad terms")
    elif avg_views < 100000:
        action_items.append("ü§ù Collab v·ªõi creators c√πng size (10K-100K views)")
        action_items.append("üìä Double down on content types c√≥ highest views")
    else:
        action_items.append("üìà Scale winning formula: T·∫°o series t·ª´ top performers")
        action_items.append("üíé Maintain quality while increasing frequency")
    
    # 2. Based on engagement
    avg_engagement = summary.get('avg_engagement_rate', 0)
    
    if avg_engagement < 1:
        action_items.append("üé¨ Re-edit videos: Hook m·∫°nh h∆°n trong 3 seconds ƒë·∫ßu")
        action_items.append("üí¨ End screen CTA: Ask specific question ƒë·ªÉ trigger comments")
    elif avg_engagement < 2:
        action_items.append("üìå Pin comment v·ªõi question ho·∫∑c poll ngay khi upload")
        action_items.append("‚ù§Ô∏è Reply t·∫•t c·∫£ comments trong 24h ƒë·∫ßu")
    elif avg_engagement < 5:
        action_items.append("üèÜ Create community posts ƒë·ªÉ maintain engagement between videos")
    
    # 3. Based on content analysis
    if transcripts:
        # Check if using CTAs
        all_text = ' '.join(t.get('full_text', '').lower() for t in transcripts[:5])
        if 'subscribe' not in all_text and 'like' not in all_text:
            action_items.append("üì¢ Add clear CTAs: 'Like v√† subscribe' ·ªü timing ph√π h·ª£p")
    
    # 4. Based on consistency
    if len(videos) > 5:
        # Simple consistency check
        recent_5 = sorted(videos, key=lambda x: x.get('published_at', ''), reverse=True)[:5]
        if recent_5:
            action_items.append("üìÖ Set fixed upload schedule v√† announce cho audience")
    
    # 5. Based on comment sentiment
    if comments:
        questions = [c for c in comments if '?' in c.get('text', '')]
        if len(questions) > len(comments) * 0.2:
            action_items.append("üìπ Create FAQ video addressing top audience questions")
            
        negative = sum(1 for c in comments if any(word in c.get('text', '').lower() 
                      for word in ['boring', 'long', 'slow']))
        if negative > len(comments) * 0.1:
            action_items.append("‚úÇÔ∏è Tighten editing: Cut dead air v√† keep pace moving")
    
    # 6. Platform specific
    if not any('short' in v.get('title', '').lower() for v in videos):
        action_items.append("üì± Start YouTube Shorts strategy: 1 short/week t·ª´ existing content")
    
    # 7. Based on top video analysis
    if videos:
        top_video = max(videos, key=lambda x: x.get('view_count', 0))
        top_engagement = ((top_video.get('like_count', 0) + top_video.get('comment_count', 0)) / 
                         top_video.get('view_count', 1)) * 100
        
        if top_engagement > avg_engagement * 2:
            action_items.append(f"üîÑ Analyze v√† replicate format c·ªßa: \"{top_video.get('title', '')[:40]}...\"")
    
    return action_items


def analyze_video_success_factors(video: Dict) -> List[str]:
    """Analyze success factors of a specific video."""
    factors = []
    
    title = video.get('title', '')
    views = video.get('view_count', 0)
    engagement = ((video.get('like_count', 0) + video.get('comment_count', 0)) / views * 100) if views > 0 else 0
    
    # Title analysis
    if '?' in title:
        factors.append("S·ª≠ d·ª•ng question hook t·∫°o curiosity")
    if any(str(i) in title for i in range(10)):
        factors.append("Number in title (specific v√† clickable)")
    if len(title) < 70:
        factors.append("Title length optimal cho c·∫£ desktop v√† mobile")
    
    # Timing analysis
    published = video.get('published_at', '')
    if published:
        # Could analyze day of week, time, etc.
        factors.append("Timing ph√π h·ª£p v·ªõi audience timezone")
    
    # Performance metrics
    if engagement > 5:
        factors.append(f"Exceptional engagement rate ({engagement:.1f}%)")
    if views > 1000000:
        factors.append("ƒê·∫°t viral threshold (1M+ views)")
        
    return factors


def generate_performance_insights(top_videos: List[Dict]) -> List[str]:
    """Generate insights from top performing videos."""
    insights = []
    
    if not top_videos:
        return insights
    
    # Average metrics of top videos
    avg_views = sum(v.get('view_count', 0) for v in top_videos) / len(top_videos)
    avg_engagement = sum(((v.get('like_count', 0) + v.get('comment_count', 0)) / 
                         v.get('view_count', 1) * 100) for v in top_videos) / len(top_videos)
    
    # Title length pattern
    title_lengths = [len(v.get('title', '')) for v in top_videos]
    avg_title_length = sum(title_lengths) / len(title_lengths)
    
    insights.append(f"Top videos average {avg_views:,.0f} views v·ªõi {avg_engagement:.1f}% engagement")
    insights.append(f"Optimal title length: {avg_title_length:.0f} characters")
    
    # Common elements
    common = analyze_common_elements(top_videos)
    if common:
        insights.append(f"Common success elements: {', '.join(common)}")
    
    # Duration pattern
    durations = [v.get('duration', '') for v in top_videos if v.get('duration')]
    if durations:
        # Could analyze optimal video length
        insights.append("Video length ph√π h·ª£p v·ªõi content type v√† audience retention")
    
    return insights


def generate_channel_specific_tips(data: Dict) -> List[str]:
    """Generate channel-specific tips based on analysis."""
    tips = []
    videos = data.get('videos', [])
    
    if not videos:
        return ["C·∫ßn th√™m d·ªØ li·ªáu ƒë·ªÉ t·∫°o tips c·ª• th·ªÉ"]
    
    # Analyze top performing video patterns
    top_videos = sorted(videos, key=lambda x: x.get('view_count', 0), reverse=True)[:3]
    
    for video in top_videos:
        title = video.get('title', '')
        # Extract patterns from successful titles
        if '?' in title:
            tips.append("S·ª≠ d·ª•ng c√¢u h·ªèi trong title ƒë·ªÉ t·∫°o curiosity")
        if any(str(i) in title for i in range(10)):
            tips.append("S·ª≠ d·ª•ng s·ªë trong title (listicles) ƒë·ªÉ thu h√∫t")
        if any(word in title.lower() for word in ['how', 'why', 'what']):
            tips.append("Ti·∫øp t·ª•c v·ªõi format educational content")
            
    return list(set(tips))  # Remove duplicates