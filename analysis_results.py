# analysis_results.py
"""
Module for analyzing Youtube data and formatting results in Vietnamese
UPDATED WITH PERFORMANCE OPTIMIZATIONS
Provides detailed insights for viral content creation
"""

import re
from typing import Dict, List, Tuple
from collections import Counter
import logging

# Import performance optimizations
try:
    from performance_config import perf_config, UIOptimizer, MemoryOptimizer
    PERFORMANCE_OPTIMIZATIONS = True
except ImportError:
    PERFORMANCE_OPTIMIZATIONS = False

logger = logging.getLogger(__name__)


def format_analysis_results(data: Dict) -> str:
    """Format analysis results in Vietnamese with detailed insights and performance optimizations."""
    if not data:
        return "KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ phÃ¢n tÃ­ch"
    
    # Apply performance optimizations if available
    if PERFORMANCE_OPTIMIZATIONS:
        # Limit display items to prevent UI lag
        max_display = perf_config.get('max_display_items', 100)
        data = MemoryOptimizer.limit_data_size(data, max_display)
    
    summary = data.get('summary', {})
    
    result_text = f"""
ğŸ“Š Káº¾T QUáº¢ PHÃ‚N TÃCH YOUTUBE CHI TIáº¾T
{'='*80}

ğŸ“ˆ Tá»”NG QUAN Dá»® LIá»†U:
ğŸ“º Sá»‘ kÃªnh phÃ¢n tÃ­ch: {summary.get('channels_analyzed', 0)}
ğŸ¬ Tá»•ng sá»‘ video: {summary.get('total_videos', 0)}
ğŸ’¬ Tá»•ng sá»‘ bÃ¬nh luáº­n: {summary.get('total_comments', 0):,}
ğŸ“ Sá»‘ transcript thu tháº­p: {summary.get('total_transcripts', 0)}
ğŸ‘ï¸ Tá»•ng lÆ°á»£t xem: {summary.get('total_views', 0):,}
ğŸ‘ Tá»•ng lÆ°á»£t thÃ­ch: {summary.get('total_likes', 0):,}
ğŸ“ˆ Tá»· lá»‡ tÆ°Æ¡ng tÃ¡c trung bÃ¬nh: {summary.get('avg_engagement_rate', 0):.2f}%

"""

    # PhÃ¢n tÃ­ch tá»«ng khÃ­a cáº¡nh vá»›i optimizations
    try:
        # Quick insights first (most important for user)
        result_text += generate_quick_insights_optimized(data)
        
        # Core analysis sections (optimized)
        result_text += analyze_content_themes_optimized(data)
        result_text += analyze_us_audience_appeal_optimized(data)
        result_text += analyze_audience_sentiment_optimized(data)
        result_text += analyze_strengths_optimized(data)
        result_text += analyze_weaknesses_optimized(data)
        result_text += suggest_viral_strategies_optimized(data)
        result_text += show_top_videos_details_optimized(data)
        
    except Exception as e:
        logger.error(f"Error in analysis: {e}")
        result_text += f"\nâŒ Lá»—i khi phÃ¢n tÃ­ch: {e}\n"
    
    # Additional requirements if any
    additional_reqs = data.get('additional_requirements', [])
    if additional_reqs:
        result_text += f"""
ğŸ“‹ PHÃ‚N TÃCH Bá»” SUNG THEO YÃŠU Cáº¦U:
{'='*80}

"""
        # Limit additional requirements to prevent overload
        display_reqs = additional_reqs[:5] if PERFORMANCE_OPTIMIZATIONS else additional_reqs
        
        for i, req in enumerate(display_reqs, 1):
            result_text += f"ğŸ” YÃªu cáº§u {i}: {req['requirement']}\n"
            result_text += f"â° Thá»i gian: {req['timestamp']}\n"
            result_text += f"ğŸ“Š Káº¿t quáº£:\n{req['analysis']}\n\n"
            
    return result_text


def generate_quick_insights_optimized(data: Dict) -> str:
    """Generate quick insights without heavy processing - OPTIMIZED VERSION"""
    insights = []
    videos = data.get('video', [])
    
    if not videos:
        return """
ğŸ’¡ INSIGHTS NHANH:
{'='*40}

â“ Cáº§n dá»¯ liá»‡u video Ä‘á»ƒ táº¡o insights chi tiáº¿t.
ğŸ¯ HÃ£y thá»­ phÃ¢n tÃ­ch vá»›i Ã­t nháº¥t 5-10 videos Ä‘á»ƒ cÃ³ káº¿t quáº£ tá»‘t hÆ¡n.

"""
    
    # Quick calculations (optimized)
    total_videos = len(videos)
    avg_views = sum(v.get('view_count', 0) for v in videos) / total_videos
    top_video = max(videos, key=lambda x: x.get('view_count', 0))
    
    # Calculate engagement distribution efficiently
    high_engagement = len([v for v in videos if v.get('engagement_rate', 0) > 5])
    good_engagement = len([v for v in videos if 2 <= v.get('engagement_rate', 0) <= 5])
    low_engagement = total_videos - high_engagement - good_engagement
    
    # Performance assessment
    if avg_views > 1000000:
        insights.append("ğŸ”¥ VIRAL POTENTIAL: Ráº¥t cao - Channel Ä‘Ã£ cÃ³ viral content")
        insights.append("ğŸ¯ Strategy: Scale winning formulas vÃ  maintain consistency")
    elif avg_views > 100000:
        insights.append("â­ VIRAL POTENTIAL: Cao - Äang trÃªn Ä‘Ã  phÃ¡t triá»ƒn tá»‘t")
        insights.append("ğŸ“ˆ Strategy: Optimize top performers vÃ  increase frequency")
    elif avg_views > 10000:
        insights.append("ğŸ“ˆ VIRAL POTENTIAL: Trung bÃ¬nh - CÃ³ cÆ¡ há»™i cáº£i thiá»‡n")
        insights.append("ğŸ”§ Strategy: A/B test formats vÃ  improve thumbnails")
    else:
        insights.append("ğŸ¯ VIRAL POTENTIAL: Äang xÃ¢y dá»±ng - Cáº§n tá»‘i Æ°u strategy")
        insights.append("ğŸš€ Strategy: Focus SEO vÃ  consistent posting")
    
    # Top performer insight
    top_views = top_video.get('view_count', 0)
    top_engagement = top_video.get('engagement_rate', 0)
    insights.append(f"ğŸ† BEST PERFORMER: {top_views:,} views ({top_engagement:.1f}% engagement)")
    
    # Engagement distribution insight
    if high_engagement > total_videos * 0.3:
        insights.append(f"âœ… ENGAGEMENT: Excellent - {high_engagement} high-performing videos")
    elif good_engagement > total_videos * 0.5:
        insights.append(f"ğŸ‘ ENGAGEMENT: Good - Consistent performance pattern")
    else:
        insights.append(f"âš ï¸ ENGAGEMENT: Needs improvement - Focus on hooks vÃ  CTAs")
    
    return f"""
ğŸ’¡ INSIGHTS NHANH:
{'='*40}

{chr(10).join(f'â€¢ {insight}' for insight in insights)}

ğŸ“Š ENGAGEMENT BREAKDOWN:
ğŸ”¥ High (>5%): {high_engagement} videos
ğŸ‘ Good (2-5%): {good_engagement} videos  
âš ï¸ Low (<2%): {low_engagement} videos

"""


def analyze_content_themes_optimized(data: Dict) -> str:
    """PhÃ¢n tÃ­ch chá»§ Ä‘á» ná»™i dung qua transcript - OPTIMIZED VERSION."""
    transcripts = data.get('transcripts', [])
    videos = data.get('video', [])
    
    result = f"""
ğŸ¯ 1. Ná»˜I DUNG CÃC VIDEO Äá»€ Cáº¬P Äáº¾N CHá»¦ Äá»€ GÃŒ?
{'='*80}

"""
    
    if not transcripts:
        result += "âŒ KhÃ´ng cÃ³ transcript Ä‘á»ƒ phÃ¢n tÃ­ch chá»§ Ä‘á».\n"
        result += "ğŸ’¡ Gá»£i Ã½: HÃ£y Ä‘áº£m báº£o video cÃ³ phá»¥ Ä‘á» Ä‘á»ƒ phÃ¢n tÃ­ch ná»™i dung tá»‘t hÆ¡n.\n\n"
        
        # PhÃ¢n tÃ­ch qua tiÃªu Ä‘á» video thay tháº¿ (optimized)
        if videos:
            result += "ğŸ“‹ PHÃ‚N TÃCH QUA TIÃŠU Äá»€ VIDEO:\n"
            titles = [video.get('title', '') for video in videos[:20]]  # Limit for performance
            title_themes = analyze_titles_for_themes_optimized(titles)
            for theme, count in list(title_themes.items())[:5]:  # Top 5 themes
                result += f"  â€¢ {theme}: {count} video\n"
        result += "\n"
        return result
    
    # Optimized theme analysis - limit processing
    sample_transcripts = transcripts[:10] if PERFORMANCE_OPTIMIZATIONS else transcripts
    all_text = ""
    video_themes = {}
    
    for transcript in sample_transcripts:
        text = transcript.get('full_text', '')
        # Limit text length for performance
        if PERFORMANCE_OPTIMIZATIONS and len(text) > 5000:
            text = text[:5000]
        all_text += text + " "
        
        # PhÃ¢n tÃ­ch tá»«ng video
        video_id = transcript.get('video_id', '')
        if video_id:
            video_themes[video_id] = analyze_text_themes_optimized(text)
    
    # Optimized keyword analysis
    psychology_keywords = {
        'ğŸ’• Má»‘i quan há»‡ & TÃ¬nh yÃªu': [
            'relationship', 'love', 'dating', 'partner', 'marriage', 'couple'
        ],
        'ğŸ§  TÃ¢m lÃ½ há»c & HÃ nh vi': [
            'psychology', 'mind', 'brain', 'behavior', 'emotion', 'feeling'
        ],
        'ğŸŒŸ PhÃ¡t triá»ƒn báº£n thÃ¢n': [
            'self', 'improve', 'growth', 'success', 'confidence', 'motivation'
        ],
        'ğŸ¥ Sá»©c khá»e tinh tháº§n': [
            'anxiety', 'depression', 'stress', 'mental health', 'therapy'
        ],
        'ğŸ’¬ Giao tiáº¿p & XÃ£ há»™i': [
            'communication', 'talk', 'speak', 'social', 'friends', 'family'
        ]
    }
    
    # Äáº¿m tá»« khÃ³a (optimized)
    theme_counts = {}
    text_lower = all_text.lower()
    
    for theme, keywords in psychology_keywords.items():
        count = sum(text_lower.count(keyword.lower()) for keyword in keywords)
        if count > 0:
            theme_counts[theme] = count
    
    # Hiá»ƒn thá»‹ káº¿t quáº£ (limited)
    if theme_counts:
        sorted_themes = sorted(theme_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        
        result += "ğŸ“‹ CHá»¦ Äá»€ CHÃNH ÄÆ¯á»¢C Äá»€ Cáº¬P:\n\n"
        
        for i, (theme, count) in enumerate(sorted_themes, 1):
            percentage = (count / sum(theme_counts.values())) * 100
            result += f"  {i}. {theme}\n"
            result += f"     ğŸ“Š Xuáº¥t hiá»‡n: {count} láº§n ({percentage:.1f}%)\n"
            
        # Sample video analysis (limited for performance)
        result += "\nğŸ“ SAMPLE VIDEO ANALYSIS:\n"
        sample_videos = videos[:3] if PERFORMANCE_OPTIMIZATIONS else videos[:5]
        
        for i, video in enumerate(sample_videos, 1):
            video_id = video.get('video_id', '')
            title = video.get('title', 'KhÃ´ng cÃ³ tiÃªu Ä‘á»')[:60] + "..."
            result += f"\nğŸ¬ Video {i}: {title}\n"
            
            if video_id in video_themes and video_themes[video_id]:
                top_theme = max(video_themes[video_id].items(), key=lambda x: x[1])
                result += f"   ğŸ¯ Chá»§ Ä‘á» chÃ­nh: {top_theme[0]} ({top_theme[1]} láº§n)\n"
            else:
                result += "   â“ Chá»§ Ä‘á» khÃ´ng xÃ¡c Ä‘á»‹nh\n"
    else:
        result += "â“ KhÃ´ng phÃ¡t hiá»‡n chá»§ Ä‘á» tÃ¢m lÃ½ rÃµ rÃ ng.\n"
        result += "ğŸ’¡ CÃ³ thá»ƒ cáº§n phÃ¢n tÃ­ch vá»›i dataset lá»›n hÆ¡n.\n"
    
    result += "\n"
    return result


def analyze_us_audience_appeal_optimized(data: Dict) -> str:
    """PhÃ¢n tÃ­ch sá»©c hÃºt vá»›i khÃ¡n giáº£ Má»¹ - OPTIMIZED VERSION."""
    result = f"""
ğŸ‡ºğŸ‡¸ 2. ÄIá»€U GÃŒ KHIáº¾N Ná»˜I DUNG THU HÃšT KHÃN GIáº¢ Má»¸?
{'='*80}

"""
    
    transcripts = data.get('transcripts', [])
    
    if not transcripts:
        result += "âŒ Cáº§n transcript Ä‘á»ƒ phÃ¢n tÃ­ch sá»©c hÃºt vá»›i khÃ¡n giáº£ Má»¹.\n\n"
        return result
    
    # Optimized appeal factors (reduced set for performance)
    us_appeal_factors = {
        'ğŸ¯ TÃ­nh cÃ¡ nhÃ¢n hÃ³a': ['you', 'your', 'yourself', 'personal'],
        'ğŸ”¥ TÃ­nh kháº©n cáº¥p': ['now', 'today', 'secret', 'hidden', 'revealed'],
        'ğŸ’ª Tá»± lá»±c cÃ¡nh sinh': ['self-made', 'control', 'power', 'strong'],
        'ğŸ§  Khoa há»c/Research': ['research', 'study', 'science', 'proven'],
        'â¤ï¸ Cáº£m xÃºc máº¡nh': ['amazing', 'incredible', 'shocking', 'love']
    }
    
    # Limit transcripts for performance
    sample_transcripts = transcripts[:5] if PERFORMANCE_OPTIMIZATIONS else transcripts
    
    # PhÃ¢n tÃ­ch transcript (optimized)
    all_text = ""
    for transcript in sample_transcripts:
        text = transcript.get('full_text', '')
        # Limit text length
        if len(text) > 3000:
            text = text[:3000]
        all_text += text + " "
    
    text_lower = all_text.lower()
    appeal_scores = {}
    
    for factor, keywords in us_appeal_factors.items():
        score = 0
        found_keywords = []
        for keyword in keywords:
            count = text_lower.count(keyword.lower())
            if count > 0:
                score += count
                found_keywords.append(f"{keyword}({count})")
        
        if score > 0:
            appeal_scores[factor] = {'score': score, 'keywords': found_keywords[:3]}  # Limit keywords shown
    
    if appeal_scores:
        sorted_appeals = sorted(appeal_scores.items(), key=lambda x: x[1]['score'], reverse=True)
        
        result += "ğŸ¯ CÃC Yáº¾U Tá» THU HÃšT KHÃN GIáº¢ Má»¸:\n\n"
        
        for i, (factor, data_info) in enumerate(sorted_appeals, 1):
            score = data_info['score']
            keywords = data_info['keywords']
            result += f"  {i}. {factor}\n"
            result += f"     ğŸ“Š Äiá»ƒm sá»‘: {score} (tá»«: {', '.join(keywords)})\n"
        
        # ÄÃ¡nh giÃ¡ tá»•ng thá»ƒ
        total_score = sum(item[1]['score'] for item in sorted_appeals)
        result += f"\nğŸ“ˆ Tá»”NG ÄIá»‚M THU HÃšT: {total_score}\n"
        
        if total_score >= 50:
            result += "âœ… Ná»™i dung cÃ³ tiá»m nÄƒng thu hÃºt tá»‘t khÃ¡n giáº£ Má»¹\n"
        elif total_score >= 20:
            result += "âš ï¸ Ná»™i dung cÃ³ potential, cáº§n optimize thÃªm\n"
        else:
            result += "âŒ Cáº§n Ä‘iá»u chá»‰nh Ä‘á»ƒ phÃ¹ há»£p khÃ¡n giáº£ Má»¹ hÆ¡n\n"
            
    else:
        result += "â“ ChÆ°a phÃ¡t hiá»‡n yáº¿u tá»‘ thu hÃºt khÃ¡n giáº£ Má»¹ rÃµ rÃ ng.\n"
        result += "ğŸ’¡ Gá»£i Ã½: ThÃªm personal stories, urgency, scientific backing.\n"
    
    result += "\n"
    return result


def analyze_audience_sentiment_optimized(data: Dict) -> str:
    """PhÃ¢n tÃ­ch cáº£m nháº­n khÃ¡n giáº£ qua comment - OPTIMIZED VERSION."""
    result = f"""
ğŸ’¬ 3. Cáº¢M NHáº¬N Cá»¦A KHÃN GIáº¢ KHI XEM VIDEO?
{'='*80}

"""
    
    comments = data.get('bÃ¬nh luáº­n', [])
    
    if not comments:
        result += "âŒ KhÃ´ng cÃ³ comment Ä‘á»ƒ phÃ¢n tÃ­ch cáº£m nháº­n khÃ¡n giáº£.\n\n"
        return result
    
    # Limit comments for performance
    sample_comments = comments[:100] if PERFORMANCE_OPTIMIZATIONS else comments
    
    # Simplified sentiment keywords
    sentiment_keywords = {
        'ğŸ˜ TÃ­ch cá»±c': ['love', 'amazing', 'great', 'awesome', 'thank', 'helpful'],
        'ğŸ˜” TiÃªu cá»±c': ['hate', 'terrible', 'bad', 'boring', 'stupid', 'waste'],
        'ğŸ¤” Tháº¯c máº¯c': ['question', 'how', 'why', 'what', 'explain'],
        'ğŸ¯ LiÃªn quan': ['me too', 'same', 'relate', 'exactly', 'my life']
    }
    
    # PhÃ¢n tÃ­ch sentiment (optimized)
    sentiment_counts = {}
    positive_comments = []
    
    for comment in sample_comments:
        text = comment.get('text', '').lower()
        
        for sentiment, keywords in sentiment_keywords.items():
            if sentiment not in sentiment_counts:
                sentiment_counts[sentiment] = 0
            
            if any(keyword in text for keyword in keywords):
                sentiment_counts[sentiment] += 1
                
                # Collect positive examples
                if sentiment == 'ğŸ˜ TÃ­ch cá»±c' and len(positive_comments) < 3:
                    positive_comments.append(comment)
    
    # Hiá»ƒn thá»‹ phÃ¢n tÃ­ch
    result += "ğŸ“Š PHÃ‚N TÃCH Cáº¢M XÃšC:\n\n"
    
    if sentiment_counts:
        total_sentiments = sum(sentiment_counts.values())
        
        for sentiment, count in sentiment_counts.items():
            if count > 0:
                percentage = (count / total_sentiments) * 100
                result += f"  {sentiment}: {count} comment ({percentage:.1f}%)\n"
        
        # ÄÃ¡nh giÃ¡ tá»•ng thá»ƒ
        positive_score = sentiment_counts.get('ğŸ˜ TÃ­ch cá»±c', 0)
        negative_score = sentiment_counts.get('ğŸ˜” TiÃªu cá»±c', 0)
        
        result += f"\nğŸ“ˆ ÄÃNH GIÃ:\n"
        if positive_score > negative_score * 2:
            result += "âœ… Pháº£n á»©ng khÃ¡n giáº£ ráº¥t tÃ­ch cá»±c\n"
        elif positive_score > negative_score:
            result += "ğŸ‘ Pháº£n á»©ng khÃ¡n giáº£ tÃ­ch cá»±c\n"
        else:
            result += "âš ï¸ Cáº§n cáº£i thiá»‡n Ä‘á»ƒ cÃ³ pháº£n á»©ng tÃ­ch cá»±c hÆ¡n\n"
    
    # Hiá»ƒn thá»‹ top comment (limited)
    if positive_comments:
        result += "\nğŸ” SAMPLE POSITIVE COMMENTS:\n\n"
        for i, comment in enumerate(positive_comments, 1):
            text = comment.get('text', '')[:100] + "..." if len(comment.get('text', '')) > 100 else comment.get('text', '')
            likes = comment.get('like_count', 0)
            
            result += f"  {i}. \"{text}\" ({likes} likes)\n"
    
    result += "\n"
    return result


def analyze_strengths_optimized(data: Dict) -> str:
    """PhÃ¢n tÃ­ch Ä‘iá»ƒm máº¡nh - OPTIMIZED VERSION."""
    result = f"""
ğŸ’ª 4. ÄIá»‚M Máº NH Cá»¦A CÃC VIDEO:
{'='*80}

"""
    
    videos = data.get('video', [])
    summary = data.get('summary', {})
    
    strengths = []
    
    # Quick strength analysis
    avg_engagement = summary.get('avg_engagement_rate', 0)
    total_videos = len(videos)
    
    if avg_engagement > 5:
        strengths.append(f"ğŸ”¥ Engagement xuáº¥t sáº¯c ({avg_engagement:.2f}%) - Top tier performance")
    elif avg_engagement > 2:
        strengths.append(f"ğŸ“ˆ Engagement tá»‘t ({avg_engagement:.2f}%) - TrÃªn má»©c TB")
    
    # Views analysis (optimized)
    if videos:
        avg_views = summary.get('avg_views', 0)
        viral_videos = [v for v in videos if v.get('view_count', 0) > 1000000]
        
        if viral_videos:
            strengths.append(f"ğŸš€ {len(viral_videos)} video viral (1M+ views)")
        elif avg_views > 100000:
            strengths.append(f"ğŸ‘ï¸ Views áº¥n tÆ°á»£ng (TB: {avg_views:,.0f}/video)")
        elif avg_views > 10000:
            strengths.append(f"ğŸ“º Performance á»•n Ä‘á»‹nh (TB: {avg_views:,.0f}/video)")
    
    # Consistency check (simplified)
    if total_videos > 5:
        high_performers = len([v for v in videos if v.get('engagement_rate', 0) > avg_engagement])
        consistency_rate = (high_performers / total_videos) * 100
        
        if consistency_rate > 50:
            strengths.append(f"ğŸ“Š Consistency tá»‘t ({consistency_rate:.0f}% videos above average)")
    
    # Top performer analysis
    if videos:
        top_video = max(videos, key=lambda x: x.get('view_count', 0))
        top_views = top_video.get('view_count', 0)
        top_engagement = top_video.get('engagement_rate', 0)
        
        if top_views > 500000 or top_engagement > 5:
            strengths.append(f"ğŸ† Best performer: {top_views:,} views, {top_engagement:.1f}% engagement")
    
    # Display strengths
    if strengths:
        for i, strength in enumerate(strengths, 1):
            result += f"  {i}. {strength}\n"
    else:
        result += "â“ Cáº§n dataset lá»›n hÆ¡n Ä‘á»ƒ xÃ¡c Ä‘á»‹nh Ä‘iá»ƒm máº¡nh.\n"
    
    # Quick recommendations
    result += f"\nğŸ¯ LEVERAGE POINTS:\n"
    if avg_engagement > 3:
        result += "â€¢ Scale successful content formats\n"
        result += "â€¢ Maintain posting consistency\n"
    else:
        result += "â€¢ Focus on improving engagement first\n"
        result += "â€¢ Analyze top performers for patterns\n"
    
    result += "\n"
    return result


def analyze_weaknesses_optimized(data: Dict) -> str:
    """PhÃ¢n tÃ­ch Ä‘iá»ƒm yáº¿u - OPTIMIZED VERSION."""
    result = f"""
âš ï¸ 5. ÄIá»‚M Háº N CHáº¾ VÃ€ Cáº¦N Cáº¢I THIá»†N:
{'='*80}

"""
    
    videos = data.get('video', [])
    summary = data.get('summary', {})
    
    weaknesses = []
    improvements = []
    
    # Quick weakness analysis
    avg_engagement = summary.get('avg_engagement_rate', 0)
    total_videos = len(videos)
    
    if avg_engagement < 1:
        weaknesses.append("ğŸ“‰ Engagement tháº¥p (<1%) - Cáº§n cáº£i thiá»‡n urgent")
        improvements.append("ğŸ’¡ Focus hook máº¡nh + clear CTAs")
    elif avg_engagement < 2:
        weaknesses.append("ğŸ“Š Engagement dÆ°á»›i TB ngÃ nh (<2%)")
        improvements.append("ğŸ’¡ A/B test thumbnails + titles")
    
    # Performance distribution analysis
    if videos:
        low_performers = len([v for v in videos if v.get('engagement_rate', 0) < 1])
        if low_performers > total_videos * 0.3:
            weaknesses.append(f"âš ï¸ {low_performers} videos cÃ³ engagement tháº¥p")
            improvements.append("ğŸ’¡ Analyze low performers Ä‘á»ƒ trÃ¡nh láº·p láº¡i")
    
    # Content gaps (simplified)
    transcripts = data.get('transcripts', [])
    if len(transcripts) < len(videos) * 0.5:
        missing_percent = ((len(videos) - len(transcripts)) / len(videos)) * 100
        weaknesses.append(f"ğŸ“ {missing_percent:.0f}% videos thiáº¿u subtitles")
        improvements.append("ğŸ’¡ Add subtitles Ä‘á»ƒ improve accessibility + SEO")
    
    # Comment engagement
    comments = data.get('bÃ¬nh luáº­n', [])
    if videos and comments:
        avg_comments_per_video = len(comments) / len(videos)
        if avg_comments_per_video < 10:
            weaknesses.append("ğŸ’¬ Ãt comment - KhÃ¡n giáº£ chÆ°a engage sÃ¢u")
            improvements.append("ğŸ’¡ End videos vá»›i specific questions")
    
    # Display results
    if weaknesses:
        result += "ğŸš¨ CÃC ÄIá»‚M Cáº¦N Cáº¢I THIá»†N:\n\n"
        for i, weakness in enumerate(weaknesses, 1):
            result += f"  {i}. {weakness}\n"
        
        result += "\nğŸ”§ HÃ€NH Äá»˜NG Æ¯U TIÃŠN:\n\n"
        for i, improvement in enumerate(improvements, 1):
            result += f"  {i}. {improvement}\n"
    else:
        result += "âœ… KhÃ´ng phÃ¡t hiá»‡n Ä‘iá»ƒm yáº¿u Ä‘Ã¡ng ká»ƒ.\n"
        result += "ğŸ’¡ Focus vÃ o scaling nhá»¯ng gÃ¬ Ä‘ang work.\n"
    
    result += "\n"
    return result


def suggest_viral_strategies_optimized(data: Dict) -> str:
    """Gá»£i Ã½ chiáº¿n lÆ°á»£c viral - OPTIMIZED VERSION."""
    result = f"""
ğŸš€ 6. CHIáº¾N LÆ¯á»¢C VIRAL - SCALE CONTENT HIá»†U QUáº¢:
{'='*80}

"""
    
    videos = data.get('video', [])
    summary = data.get('summary', {})
    
    if not videos:
        result += "â“ Cáº§n data Ä‘á»ƒ táº¡o strategy cá»¥ thá»ƒ.\n\n"
        return result
    
    # Quick strategy based on current performance
    avg_views = summary.get('avg_views', 0)
    avg_engagement = summary.get('avg_engagement_rate', 0)
    
    result += "ğŸ“ STRATEGY Dá»°A TRÃŠN PERFORMANCE HIá»†N Táº I:\n\n"
    
    # Content strategy based on data
    if avg_views > 100000:
        result += "ğŸ¯ HIGH PERFORMER STRATEGY:\n"
        result += "â€¢ Analyze top 3 videos Ä‘á»ƒ tÃ¬m success patterns\n"
        result += "â€¢ Scale winning formats vá»›i variations\n"
        result += "â€¢ Increase posting frequency\n"
        result += "â€¢ Cross-promote lÃªn multiple platforms\n\n"
    elif avg_views > 10000:
        result += "ğŸ“ˆ GROWTH STRATEGY:\n"
        result += "â€¢ Optimize thumbnails vá»›i A/B testing\n"
        result += "â€¢ Improve titles vá»›i emotional triggers\n"
        result += "â€¢ Focus SEO vá»›i trending keywords\n"
        result += "â€¢ Create YouTube Shorts cho discovery\n\n"
    else:
        result += "ğŸ¯ FOUNDATION STRATEGY:\n"
        result += "â€¢ Establish consistent posting schedule\n"
        result += "â€¢ Focus basic SEO optimization\n"
        result += "â€¢ Build audience vá»›i engaging hooks\n"
        result += "â€¢ Study competitor successful content\n\n"
    
    # Engagement optimization
    result += "ğŸ’¡ ENGAGEMENT OPTIMIZATION:\n"
    if avg_engagement > 3:
        result += "â€¢ Maintain current engagement tactics\n"
        result += "â€¢ Test longer content formats\n"
    else:
        result += "â€¢ Strengthen hooks trong 3-5 giÃ¢y Ä‘áº§u\n"
        result += "â€¢ Add clear CTAs throughout video\n"
        result += "â€¢ Pin engaging comments Ä‘á»ƒ trigger responses\n"
    
    # Quick win tactics
    result += "\nâš¡ QUICK WIN TACTICS:\n"
    result += "â€¢ Create 3 Shorts from best moments cá»§a top video\n"
    result += "â€¢ Update old video titles vá»›i current trends\n"
    result += "â€¢ Engage vá»›i táº¥t cáº£ comments trong 24h Ä‘áº§u\n"
    result += "â€¢ Cross-promote trÃªn social media platforms\n"
    
    # Performance targets
    current_avg = int(avg_views)
    if current_avg > 0:
        target_views = min(current_avg * 2, current_avg + 50000)  # Realistic targets
        target_engagement = min(avg_engagement * 1.5, avg_engagement + 2)
        target_subscribers = max(100, int(current_avg/100))
        
        result += f"\nğŸ“Š TARGET CHO 30 NGÃ€Y Tá»šI:\n"
        result += f"â€¢ Avg views: {current_avg:,} â†’ {target_views:,}\n"
        result += f"â€¢ Engagement: {avg_engagement:.1f}% â†’ {target_engagement:.1f}%\n"
        result += f"â€¢ New subscribers: +{target_subscribers:,}\n"
    
    result += "\n"
    return result


def show_top_videos_details_optimized(data: Dict) -> str:
    """Hiá»ƒn thá»‹ chi tiáº¿t top videos - OPTIMIZED VERSION."""
    result = f"""
ğŸ† 7. TOP VIDEOS PERFORMANCE:
{'='*80}

"""
    
    videos = data.get('video', [])
    
    if not videos:
        result += "âŒ KhÃ´ng cÃ³ video data Ä‘á»ƒ phÃ¢n tÃ­ch.\n"
        return result
    
    # Limit to top 3 for performance
    display_count = 3 if PERFORMANCE_OPTIMIZATIONS else 5
    top_videos = sorted(videos, key=lambda x: x.get('view_count', 0), reverse=True)[:display_count]
    
    for i, video in enumerate(top_videos, 1):
        title = video.get('title', 'No title')
        # Truncate long titles
        if len(title) > 60:
            title = title[:57] + "..."
            
        views = video.get('view_count', 0)
        likes = video.get('like_count', 0)
        comments = video.get('comment_count', 0)
        engagement = video.get('engagement_rate', 0)
        published = video.get('published_at', '')[:10]
        
        result += f"ğŸ¬ TOP {i}: {title}\n"
        result += f"   ğŸ“Š {views:,} views | {likes:,} likes | {comments:,} comments\n"
        result += f"   ğŸ“ˆ {engagement:.2f}% engagement"
        
        if published:
            result += f" | ğŸ“… {published}"
        result += "\n"
        
        # Performance assessment
        if engagement > 5:
            result += "   ğŸ”¥ Exceptional performance\n"
        elif engagement > 2:
            result += "   â­ Above average performance\n"
        else:
            result += "   ğŸ“Š Standard performance\n"
        
        # Quick success factors for top video only
        if i == 1 and engagement > 2:
            result += "   ğŸ”‘ Success factors:\n"
            factors = analyze_video_success_factors_optimized(video)
            for factor in factors[:3]:  # Limit factors shown
                result += f"      â€¢ {factor}\n"
        
        result += "\n"
    
    # Overall insights (simplified)
    if len(videos) > display_count:
        remaining = len(videos) - display_count
        result += f"ğŸ’¡ ... and {remaining} more videos analyzed\n"
        
        # Quick pattern analysis
        avg_top_engagement = sum(v.get('engagement_rate', 0) for v in top_videos) / len(top_videos)
        result += f"ğŸ“ˆ Top performers average: {avg_top_engagement:.1f}% engagement\n"
    
    return result


# Helper functions (optimized versions)
def analyze_text_themes_optimized(text: str) -> Dict[str, int]:
    """PhÃ¢n tÃ­ch chá»§ Ä‘á» tá»« text - OPTIMIZED."""
    # Simplified themes for performance
    themes = {
        'TÃ¢m lÃ½': ['psychology', 'mind', 'mental'],
        'Quan há»‡': ['relationship', 'love', 'partner'],
        'PhÃ¡t triá»ƒn': ['improve', 'growth', 'success'],
        'Cáº£m xÃºc': ['emotion', 'feel', 'happy']
    }
    
    text_lower = text.lower()
    theme_counts = {}
    
    for theme, keywords in themes.items():
        count = sum(text_lower.count(keyword) for keyword in keywords)
        if count > 0:
            theme_counts[theme] = count
    
    return theme_counts


def analyze_titles_for_themes_optimized(titles: List[str]) -> Dict[str, int]:
    """PhÃ¢n tÃ­ch chá»§ Ä‘á» tá»« titles - OPTIMIZED."""
    all_titles = ' '.join(titles).lower()
    
    themes = {
        'Psychology': ['psychology', 'mind', 'mental'],
        'Relationships': ['relationship', 'love', 'dating'],
        'Self-help': ['improve', 'success', 'tips'],
        'Lifestyle': ['life', 'daily', 'routine']
    }
    
    theme_counts = {}
    for theme, keywords in themes.items():
        count = sum(all_titles.count(keyword) for keyword in keywords)
        if count > 0:
            theme_counts[theme] = count
    
    return theme_counts


def analyze_video_success_factors_optimized(video: Dict) -> List[str]:
    """Analyze success factors - OPTIMIZED."""
    factors = []
    
    title = video.get('title', '')
    views = video.get('view_count', 0)
    engagement = video.get('engagement_rate', 0)
    
    # Quick factor analysis
    if '?' in title:
        factors.append("Question hook creates curiosity")
    if any(str(i) in title for i in range(10)):
        factors.append("Numbers in title (specific content)")
    if engagement > 5:
        factors.append(f"High engagement rate ({engagement:.1f}%)")
    if views > 1000000:
        factors.append("Viral reach (1M+ views)")
    
    return factors


# Performance monitoring decorator
def monitor_analysis_performance(func):
    """Decorator to monitor analysis performance"""
    def wrapper(*args, **kwargs):
        if PERFORMANCE_OPTIMIZATIONS:
            import time
            start_time = time.time()
            
            try:
                result = func(*args, **kwargs)
                return result
            finally:
                elapsed = time.time() - start_time
                if elapsed > 2.0:  # Log slow operations
                    logger.warning(f"Slow analysis operation: {func.__name__} took {elapsed:.2f}s")
        else:
            return func(*args, **kwargs)
    return wrapper


# Apply monitoring to main function
format_analysis_results = monitor_analysis_performance(format_analysis_results)


# Additional utility functions for the original implementation
def analyze_content_themes(data: Dict) -> str:
    """Original function for backward compatibility"""
    return analyze_content_themes_optimized(data)


def analyze_us_audience_appeal(data: Dict) -> str:
    """Original function for backward compatibility"""
    return analyze_us_audience_appeal_optimized(data)


def analyze_audience_sentiment(data: Dict) -> str:
    """Original function for backward compatibility"""
    return analyze_audience_sentiment_optimized(data)


def analyze_strengths(data: Dict) -> str:
    """Original function for backward compatibility"""
    return analyze_strengths_optimized(data)


def analyze_weaknesses(data: Dict) -> str:
    """Original function for backward compatibility"""
    return analyze_weaknesses_optimized(data)


def suggest_viral_strategies(data: Dict) -> str:
    """Original function for backward compatibility"""
    return suggest_viral_strategies_optimized(data)


def show_top_videos_details(data: Dict) -> str:
    """Original function for backward compatibility"""
    return show_top_videos_details_optimized(data)


def generate_dynamic_action_items(data: Dict) -> List[str]:
    """Generate dynamic action items based on analysis."""
    action_items = []
    videos = data.get('video', [])
    comments = data.get('bÃ¬nh luáº­n', [])
    transcripts = data.get('transcripts', [])
    summary = data.get('summary', {})
    
    if not videos:
        return ["ğŸ“Š Thu tháº­p data Ä‘á»ƒ cÃ³ insights cá»¥ thá»ƒ", 
                "ğŸ¯ Báº¯t Ä‘áº§u vá»›i 5 test videos khÃ¡c nhau"]
    
    # 1. Based on view performance
    avg_views = summary.get('avg_views', 0)
    
    if avg_views < 1000:
        action_items.append("ğŸ” NghiÃªn cá»©u SEO: Optimize title, tags, descriptions vá»›i target keywords")
        action_items.append("ğŸ–¼ï¸ Redesign thumbnails: A/B test vá»›i colors vÃ  text rÃµ rÃ ng")
    elif avg_views < 10000:
        action_items.append("ğŸ“± Táº¡o YouTube Shorts tá»« best moments cá»§a long videos")
        action_items.append("ğŸ¯ Target specific niche keywords thay vÃ¬ broad terms")
    elif avg_views < 100000:
        action_items.append("ğŸ¤ Collab vá»›i creators cÃ¹ng size (10K-100K views)")
        action_items.append("ğŸ“Š Double down on content types cÃ³ highest views")
    else:
        action_items.append("ğŸ“ˆ Scale winning formula: Táº¡o series tá»« top performers")
        action_items.append("ğŸ’ Maintain quality while increasing frequency")
    
    # 2. Based on engagement
    avg_engagement = summary.get('avg_engagement_rate', 0)
    
    if avg_engagement < 1:
        action_items.append("ğŸ¬ Re-edit videos: Hook máº¡nh hÆ¡n trong 3 seconds Ä‘áº§u")
        action_items.append("ğŸ’¬ End screen CTA: Ask specific question Ä‘á»ƒ trigger comments")
    elif avg_engagement < 2:
        action_items.append("ğŸ“Œ Pin comment vá»›i question hoáº·c poll ngay khi upload")
        action_items.append("â¤ï¸ Reply táº¥t cáº£ comments trong 24h Ä‘áº§u")
    elif avg_engagement < 5:
        action_items.append("ğŸ† Create community posts Ä‘á»ƒ maintain engagement between videos")
    
    # 3. Based on content analysis
    if transcripts:
        # Check if using CTAs
        all_text = ' '.join(t.get('full_text', '').lower() for t in transcripts[:5])
        if 'subscribe' not in all_text and 'like' not in all_text:
            action_items.append("ğŸ“¢ Add clear CTAs: 'Like vÃ  subscribe' á»Ÿ timing phÃ¹ há»£p")
    
    # 4. Based on consistency
    if len(videos) > 5:
        action_items.append("ğŸ“… Set fixed upload schedule vÃ  announce cho audience")
    
    # 5. Based on comment sentiment
    if comments:
        questions = [c for c in comments if '?' in c.get('text', '')]
        if len(questions) > len(comments) * 0.2:
            action_items.append("ğŸ“¹ Create FAQ video addressing top audience questions")
            
        negative = sum(1 for c in comments if any(word in c.get('text', '').lower() 
                      for word in ['boring', 'long', 'slow']))
        if negative > len(comments) * 0.1:
            action_items.append("âœ‚ï¸ Tighten editing: Cut dead air vÃ  keep pace moving")
    
    # 6. Platform specific
    if not any('short' in v.get('title', '').lower() for v in videos):
        action_items.append("ğŸ“± Start YouTube Shorts strategy: 1 short/week tá»« existing content")
    
    # 7. Based on top video analysis
    if videos:
        top_video = max(videos, key=lambda x: x.get('view_count', 0))
        top_engagement = ((top_video.get('like_count', 0) + top_video.get('comment_count', 0)) / 
                         top_video.get('view_count', 1)) * 100
        
        avg_engagement = summary.get('avg_engagement_rate', 0)
        if top_engagement > avg_engagement * 2:
            action_items.append(f"ğŸ”„ Analyze vÃ  replicate format cá»§a: \"{top_video.get('title', '')[:40]}...\"")
    
    return action_items[:8]  # Limit to 8 action items


def generate_channel_specific_tips(data: Dict) -> List[str]:
    """Generate channel-specific tips based on analysis."""
    tips = []
    videos = data.get('video', [])
    
    if not videos:
        return ["Cáº§n thÃªm dá»¯ liá»‡u Ä‘á»ƒ táº¡o tips cá»¥ thá»ƒ"]
    
    # Analyze top performing video patterns
    top_videos = sorted(videos, key=lambda x: x.get('view_count', 0), reverse=True)[:3]
    
    for video in top_videos:
        title = video.get('title', '')
        # Extract patterns from successful titles
        if '?' in title:
            tips.append("Sá»­ dá»¥ng cÃ¢u há»i trong title Ä‘á»ƒ táº¡o curiosity")
        if any(str(i) in title for i in range(10)):
            tips.append("Sá»­ dá»¥ng sá»‘ trong title (listicles) Ä‘á»ƒ thu hÃºt")
        if any(word in title.lower() for word in ['how', 'why', 'what']):
            tips.append("Tiáº¿p tá»¥c vá»›i format educational content")
            
    return list(set(tips))  # Remove duplicates